# -*- coding: utf-8 -*-
"""
Opentronsåè®®ç”Ÿæˆå™¨ - LangGraph
======================================

ç”¨äºè‡ªåŠ¨ç”ŸæˆOpentronsæœºå™¨äººåè®®ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. æ ¹æ®ç”¨æˆ·ç›®æ ‡ç”Ÿæˆæ ‡å‡†æ“ä½œç¨‹åº(SOP)
2. å°†SOPè½¬æ¢ä¸ºå¯æ‰§è¡Œçš„Pythonä»£ç 
3. è‡ªåŠ¨éªŒè¯å’Œä¿®å¤ä»£ç é”™è¯¯
4. æä¾›æµå¼ç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–

ä½œè€…: Gaoyuan

"""

import os
import requests
import re # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œæå–é”™è¯¯ä¿¡æ¯
import ast # ç”¨äºå¿«é€ŸPythonè¯­æ³•æ£€æŸ¥
import json # ç”¨äºå¤„ç†Plannerè¿”å›çš„JSONæ ¼å¼ä¿®æ”¹è®¡åˆ’
from typing import Optional, Callable, Dict, Any, TypedDict, Annotated, Literal
from datetime import datetime  # ç”¨äºç»™æµå¼äº‹ä»¶æ·»åŠ æ—¶é—´æˆ³
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langgraph.graph import StateGraph, END, START
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langgraph.graph.message import add_messages

# Use absolute imports from project root
from backend.config import (
    api_key, base_url, model_name,
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, DEEPSEEK_INTENT_MODEL, # Import new config
    LABWARE_FOR_OT2, LABWARE_FOR_FLEX,
    INSTRUMENTS_FOR_OT2, INSTRUMENTS_FOR_FLEX,
    MODULES_FOR_OT2, MODULES_FOR_FLEX,
    CODE_EXAMPLES, COMMON_PITFALLS_OT2
)
from backend.diff_utils import apply_diff
from backend.opentrons_utils import run_opentrons_simulation, SimulateToolInput
from backend.prompts import (
    SOP_GENERATION_PROMPT_TEMPLATE, 
    CODE_GENERATION_PROMPT_TEMPLATE_FLEX,
    CODE_GENERATION_PROMPT_TEMPLATE_OT2,
    CODE_CORRECTION_DIFF_TEMPLATE_FLEX,
    CODE_CORRECTION_DIFF_TEMPLATE_OT2,
    CODE_PLANNER_PROMPT_TEMPLATE, # æ–°å¢ï¼šPlanneræ¨¡æ¿
    CODE_DIFFER_PROMPT_TEMPLATE, # æ–°å¢ï¼šDifferæ¨¡æ¿  
    CODE_DIFFER_FIX_PROMPT_TEMPLATE, # æ–°å¢ï¼šDifferä¿®å¤æ¨¡æ¿
    # English Prompts
    ENG_SOP_GENERATION_PROMPT_TEMPLATE,
    ENG_CODE_GENERATION_PROMPT_TEMPLATE_FLEX,
    ENG_CODE_GENERATION_PROMPT_TEMPLATE_OT2,
    ENG_CODE_CORRECTION_DIFF_TEMPLATE_FLEX,
    ENG_CODE_CORRECTION_DIFF_TEMPLATE_OT2,
    ENG_CODE_PLANNER_PROMPT_TEMPLATE,
    ENG_CODE_DIFFER_PROMPT_TEMPLATE,
    ENG_SOP_CONVERSATION_CLASSIFIER_PROMPT_TEMPLATE,
    ENG_CODE_CONVERSATION_CLASSIFIER_PROMPT_TEMPLATE,
    ENG_GENERAL_CODE_CHAT_PROMPT_TEMPLATE,
)

# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰éƒ¨åˆ†
# ============================================================================

class CodeGenerationState(TypedDict):
    """
    LangGraphçŠ¶æ€ç®¡ç†ç±»
    
    è¿™ä¸ªç±»å®šä¹‰äº†ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­éœ€è¦è·Ÿè¸ªçš„æ‰€æœ‰çŠ¶æ€ä¿¡æ¯ã€‚
    LangGraphä½¿ç”¨è¿™ä¸ªçŠ¶æ€åœ¨ä¸åŒçš„å¤„ç†èŠ‚ç‚¹ä¹‹é—´ä¼ é€’æ•°æ®ã€‚
    
    å±æ€§è¯´æ˜:
        original_sop (str): åŸå§‹çš„æ ‡å‡†æ“ä½œç¨‹åºæ–‡æœ¬ï¼Œåœ¨æ•´ä¸ªæµç¨‹ä¸­ä¸ä¼šæ”¹å˜
        hardware_context (str): ç¡¬ä»¶é…ç½®ä¿¡æ¯ï¼ŒåŒ…æ‹¬æœºå™¨äººå‹å·ã€ç§»æ¶²å™¨ç­‰
        python_code (Optional[str]): å½“å‰ç‰ˆæœ¬çš„Pythonä»£ç ï¼Œä¼šé€šè¿‡diffè¿›è¡Œè¿­ä»£æ›´æ–°
        llm_diff_output (Optional[str]): LLMç”Ÿæˆçš„åŸå§‹diffæ–‡æœ¬ï¼Œç”¨äºæ—¥å¿—å’Œè°ƒè¯•
        simulation_result (Optional[dict]): æ¨¡æ‹Ÿè¿è¡Œçš„ç»“æœï¼ŒåŒ…å«æˆåŠŸ/å¤±è´¥ä¿¡æ¯
        feedback_for_llm (Dict[str, str]): ç»™å¤§è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–åé¦ˆä¿¡æ¯ï¼Œç”¨äºé”™è¯¯ä¿®æ­£
        attempts (int): å½“å‰å°è¯•æ¬¡æ•°ï¼Œç”¨äºæ§åˆ¶é‡è¯•é€»è¾‘
        max_attempts (int): æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œé¿å…æ— é™å¾ªç¯
        iteration_reporter (Optional[Callable]): å›è°ƒå‡½æ•°ï¼Œç”¨äºå‘å‰ç«¯æŠ¥å‘Šè¿›åº¦
    """
    # è¾“å…¥æ•°æ® - åœ¨è¿è¡Œè¿‡ç¨‹ä¸­ä¸ä¼šæ”¹å˜
    original_sop: str
    hardware_context: str
    
    # ä¼šè¢«æ›´æ–°çš„æ•°æ®
    python_code: Optional[str]
    llm_diff_output: Optional[str]
    simulation_result: Optional[dict]
    feedback_for_llm: Dict[str, str]
    
    # æ§åˆ¶æµç¨‹çš„å˜é‡
    attempts: int
    max_attempts: int
    
    # ç”¨äºæŠ¥å‘Šè¿›åº¦çš„å›è°ƒå‡½æ•°
    iteration_reporter: Optional[Callable[[Dict[str, Any]], None]]

# ============================================================================
# SOPç”ŸæˆåŠŸèƒ½éƒ¨åˆ†
# ============================================================================

def generate_sop_with_langchain(user_goal_with_hardware_context: str) -> str:
    """
    ä½¿ç”¨æœ¬åœ°LangChainç”Ÿæˆæ ‡å‡†æ“ä½œç¨‹åº(SOP)
    
    è¿™ä¸ªå‡½æ•°æ¥æ”¶ç”¨æˆ·çš„å®éªŒç›®æ ‡å’Œç¡¬ä»¶é…ç½®ï¼Œç„¶åä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹
    ç”Ÿæˆè¯¦ç»†çš„ã€å¯æ‰§è¡Œçš„æ ‡å‡†æ“ä½œç¨‹åºã€‚
    
    å‚æ•°:
        user_goal_with_hardware_context (str): ç»„åˆè¾“å…¥ï¼ŒåŒ…å«ç¡¬ä»¶é…ç½®å’Œç”¨æˆ·ç›®æ ‡ï¼Œ
                                              ç”¨"---"åˆ†éš”
    
    è¿”å›:
        str: ç”Ÿæˆçš„SOP markdownæ–‡æœ¬ï¼Œæˆ–è€…é”™è¯¯ä¿¡æ¯
    
    å·¥ä½œæµç¨‹:
        1. è§£æè¾“å…¥ï¼Œåˆ†ç¦»ç¡¬ä»¶é…ç½®å’Œç”¨æˆ·ç›®æ ‡
        2. è°ƒç”¨SOPç”Ÿæˆé“¾(sop_generation_chain)
        3. æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        4. å¤„ç†å¯èƒ½å‡ºç°çš„å¼‚å¸¸
    """
    try:
        # æ­¥éª¤1: åˆ†å‰²è¾“å…¥ä»¥æå–ç¡¬ä»¶é…ç½®å’Œç”¨æˆ·ç›®æ ‡
        if "---" in user_goal_with_hardware_context:
            parts = user_goal_with_hardware_context.split("---", 1)
            hardware_context = parts[0].strip()
            user_goal = parts[1].strip()
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œå‡è®¾æ•´ä¸ªè¾“å…¥éƒ½æ˜¯ç”¨æˆ·ç›®æ ‡
            hardware_context = "No specific hardware configuration provided."
            user_goal = user_goal_with_hardware_context.strip()
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œå¸®åŠ©å¼€å‘è€…äº†è§£å¤„ç†è¿‡ç¨‹
        print(f"Debug - [generate_sop_with_langchain] åŸå§‹è¾“å…¥é•¿åº¦: {len(user_goal_with_hardware_context)}")
        print(f"Debug - [generate_sop_with_langchain] ç¡¬ä»¶é…ç½®é•¿åº¦: {len(hardware_context)}")
        print(f"Debug - [generate_sop_with_langchain] ç¡¬ä»¶é…ç½®å†…å®¹:\n{hardware_context}")
        print(f"Debug - [generate_sop_with_langchain] ç”¨æˆ·ç›®æ ‡: {user_goal}")
        
        # æ­¥éª¤2: ä½¿ç”¨æœ¬åœ°LangChainç”ŸæˆSOP
        print(f"Debug - [generate_sop_with_langchain] å¼€å§‹ä½¿ç”¨æœ¬åœ°LangChainç”ŸæˆSOP")
        
        # è°ƒç”¨é¢„å…ˆé…ç½®çš„SOPç”Ÿæˆé“¾
        sop_result = sop_generation_chain.run({
            "hardware_context": hardware_context,
            "user_goal": user_goal
        })
        
        print(f"Debug - [generate_sop_with_langchain] SOPç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(sop_result)} å­—ç¬¦")
        
        # æ­¥éª¤3: ç¡®ä¿è¿”å›æ ¼å¼ä¸€è‡´
        # å¦‚æœç”Ÿæˆçš„SOPæ²¡æœ‰æ ‡å‡†æ ‡é¢˜ï¼Œè‡ªåŠ¨æ·»åŠ 
        if not sop_result.startswith("## Generated Standard Operating Procedure"):
            sop_result = f"## Generated Standard Operating Procedure (SOP)\n\n{sop_result}"
        
        return sop_result
        
    except Exception as e:
        # æ­¥éª¤4: é”™è¯¯å¤„ç† - æ•è·å¹¶è®°å½•æ‰€æœ‰å¼‚å¸¸
        print(f"Debug - [generate_sop_with_langchain] SOPç”Ÿæˆå¼‚å¸¸: {e}")
        import traceback
        error_traceback = traceback.format_exc()
        print(f"Debug - [generate_sop_with_langchain] å®Œæ•´é”™è¯¯å †æ ˆ: {error_traceback}")
        # è¿”å›æ ‡å‡†åŒ–çš„é”™è¯¯å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æŠ›å‡ºåŸå§‹å¼‚å¸¸
        return f"Error: An unexpected error occurred during SOP generation. Details: {str(e)}\nTraceback:\n{error_traceback}"

# ============================================================================
# å¤§è¯­è¨€æ¨¡å‹é…ç½®éƒ¨åˆ†
# ============================================================================

# LLM for complex generation tasks (SOPs)
llm = ChatOpenAI(
    model_name=model_name,
    openai_api_base=base_url,
    openai_api_key=api_key,
    temperature=0.0,
    streaming=True, 
    max_retries=2,
    request_timeout=60
)

# LLM for faster code generation and correction tasks
code_gen_llm = ChatOpenAI(
    model_name=DEEPSEEK_INTENT_MODEL, # Re-using the intent model name, "DeepSeek-V3-Fast"
    openai_api_base=DEEPSEEK_BASE_URL,
    openai_api_key=DEEPSEEK_API_KEY,
    temperature=0.0,
    streaming=False, # Code generation should not be streaming token by token in the backend
    max_retries=2,
    request_timeout=120 # Give more time for code generation
)

# ============================================================================
# æç¤ºè¯æ¨¡æ¿å¯¹è±¡åˆ›å»º
# ============================================================================

# åˆ›å»ºSOPç”Ÿæˆçš„æç¤ºè¯æ¨¡æ¿å¯¹è±¡
SOP_GENERATION_PROMPT = PromptTemplate(
    input_variables=["hardware_context", "user_goal"],  # è¾“å…¥å˜é‡å
    template=ENG_SOP_GENERATION_PROMPT_TEMPLATE             # æ¨¡æ¿å†…å®¹
)

# ä¸ºFlexåˆ›å»ºä»£ç ç”Ÿæˆçš„æç¤ºè¯æ¨¡æ¿å¯¹è±¡
CODE_GEN_PROMPT_FLEX = PromptTemplate(
    input_variables=["hardware_context", "sop_text", "feedback_for_llm",
                     "valid_labware_list_str", "valid_instrument_list_str", 
                     "valid_module_list_str", "code_examples_str", "previous_code",
                     "apiLevel"],
    template=ENG_CODE_GENERATION_PROMPT_TEMPLATE_FLEX
)

# ä¸ºOT-2åˆ›å»ºä»£ç ç”Ÿæˆçš„æç¤ºè¯æ¨¡æ¿å¯¹è±¡
CODE_GEN_PROMPT_OT2 = PromptTemplate(
    input_variables=["hardware_context", "sop_text", "feedback_for_llm",
                     "valid_labware_list_str", "valid_instrument_list_str", 
                     "valid_module_list_str", "code_examples_str", "previous_code",
                     "apiLevel", "common_pitfalls_str"],
    template=ENG_CODE_GENERATION_PROMPT_TEMPLATE_OT2
)

# åˆ›å»ºç”¨äºç”Ÿæˆä»£ç ä¿®æ­£Diffçš„æç¤ºè¯æ¨¡æ¿å¯¹è±¡
CODE_CORRECTION_PROMPT_FLEX = PromptTemplate(
    input_variables=[
        "analysis_of_failure", "recommended_action", "full_error_log", "previous_code",
        "valid_labware_list_str", "valid_instrument_list_str", "valid_module_list_str"
    ],
    template=ENG_CODE_CORRECTION_DIFF_TEMPLATE_FLEX
)

# ä¸ºOT-2åˆ›å»ºä»£ç ä¿®æ­£Diffçš„æç¤ºè¯æ¨¡æ¿å¯¹è±¡
CODE_CORRECTION_PROMPT_OT2 = PromptTemplate(
    input_variables=[
        "analysis_of_failure", "recommended_action", "full_error_log", "previous_code",
        "valid_labware_list_str", "valid_instrument_list_str", "valid_module_list_str"
    ],
    template=ENG_CODE_CORRECTION_DIFF_TEMPLATE_OT2
)

# ============================================================================
# LangChainé“¾å¼å¤„ç†é…ç½®éƒ¨åˆ†
# ============================================================================

# åˆå§‹åŒ–SOPç”Ÿæˆé“¾ (uses powerful 'llm' instance)
sop_generation_chain = LLMChain(llm=llm, prompt=SOP_GENERATION_PROMPT)

# åˆå§‹åŒ–ä»£ç ç”Ÿæˆé“¾ (ä¸ºFlexå’ŒOT-2åˆ†åˆ«åˆ›å»º)
code_gen_chain_flex = LLMChain(llm=code_gen_llm, prompt=CODE_GEN_PROMPT_FLEX)
code_gen_chain_ot2 = LLMChain(llm=code_gen_llm, prompt=CODE_GEN_PROMPT_OT2)

# åˆå§‹åŒ–ä»£ç ä¿®æ­£é“¾ (ä¸ºFlexå’ŒOT-2åˆ†åˆ«åˆ›å»º)
code_correction_chain_flex = LLMChain(llm=code_gen_llm, prompt=CODE_CORRECTION_PROMPT_FLEX)
code_correction_chain_ot2 = LLMChain(llm=code_gen_llm, prompt=CODE_CORRECTION_PROMPT_OT2)

# ============================================================================
# æµå¼ç”ŸæˆåŠŸèƒ½éƒ¨åˆ†
# ============================================================================

async def generate_sop_with_langchain_stream(hardware_context: str, user_goal: str):
    """
    ä½¿ç”¨LangChainä»¥æµå¼æ–¹å¼å¼‚æ­¥ç”ŸæˆSOP
    
    è¿™ä¸ªå‡½æ•°å®ç°äº†çœŸæ­£çš„æµå¼è¾“å‡ºï¼Œèƒ½å¤Ÿå®æ—¶æ˜¾ç¤ºLLMç”Ÿæˆçš„æ¯ä¸ªtokenï¼Œ
    è€Œä¸æ˜¯ç­‰å¾…å®Œæ•´ç»“æœã€‚è¿™å¤§å¤§æ”¹å–„äº†ç”¨æˆ·ä½“éªŒï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆã€‚
    
    å‚æ•°:
        hardware_context (str): ç¡¬ä»¶é…ç½®ä¿¡æ¯
        user_goal (str): ç”¨æˆ·çš„å®éªŒç›®æ ‡
    
    ç”Ÿæˆå™¨è¿”å›:
        str: æ¯æ¬¡yieldä¸€ä¸ªtokenå­—ç¬¦ä¸²
    
    æŠ€æœ¯ç»†èŠ‚:
        - ä½¿ç”¨async/awaitå®ç°å¼‚æ­¥å¤„ç†
        - ç›´æ¥è°ƒç”¨llm.astream()ç»•è¿‡LLMChainçš„ç¼“å†²
        - æ¯ä¸ªtokenç«‹å³yieldç»™è°ƒç”¨è€…
    """
    print("Debug - [generate_sop_with_langchain_stream] å¼€å§‹ä½¿ç”¨LLM astream å®æ—¶ç”ŸæˆSOP")
    
    try:
        # å‡†å¤‡é“¾çš„è¾“å…¥å‚æ•°
        chain_input = {"hardware_context": hardware_context, "user_goal": user_goal}
        
        # ä¸ºäº†å®ç°çœŸæ­£çš„tokençº§æµå¼è¾“å‡ºï¼Œæˆ‘ä»¬ç»•è¿‡LLMChainï¼Œç›´æ¥è°ƒç”¨llm.astream
        # æ­¥éª¤1: æ‰‹åŠ¨æ ¼å¼åŒ–æç¤ºè¯
        formatted_prompt = SOP_GENERATION_PROMPT.format(**chain_input)
        
        print(f"Debug - [stream] Promptå·²æ ¼å¼åŒ–ï¼Œå‡†å¤‡ç›´æ¥è°ƒç”¨llm.astream")
        
        # æ­¥éª¤2: ç›´æ¥è°ƒç”¨llm.astreamï¼Œå®ƒè¿”å›ä¸€ä¸ªåŒ…å«AIMessageChunkçš„å¼‚æ­¥è¿­ä»£å™¨
        token_count = 0
        async for chunk in llm.astream(formatted_prompt):
            # AIMessageChunkæœ‰ä¸€ä¸ª.contentå±æ€§ï¼ŒåŒ…å«å®é™…çš„tokenå­—ç¬¦ä¸²
            if chunk and hasattr(chunk, 'content') and chunk.content:
                token_count += 1
                # print(f"Debug - [stream] Yielding token #{token_count}")
                yield chunk.content  # ç«‹å³yieldæ¯ä¸ªtoken
        
        print(f"Debug - [generate_sop_with_langchain_stream] æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»å…±äº§å‡º {token_count} ä¸ªtoken")
        
    except Exception as e:
        print(f"Error - [generate_sop_with_langchain_stream] æµå¼ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")
        # é”™è¯¯æ—¶æä¾›ä¸€ä¸ªå›é€€ä¿¡æ¯
        yield f"Error: Streaming failed. Details: {str(e)}"

# ============================================================================
# é”™è¯¯åˆ†æåŠŸèƒ½éƒ¨åˆ†
# ============================================================================

def extract_error_from_simulation(simulation_output: str) -> str:
    """
    ä»æ¨¡æ‹Ÿè¾“å‡ºä¸­æ™ºèƒ½æå–ç›¸å…³é”™è¯¯ä¿¡æ¯
    
    å½“Opentronsæ¨¡æ‹Ÿè¿è¡Œå¤±è´¥æ—¶ï¼Œè¾“å‡ºé€šå¸¸åŒ…å«å¤§é‡ä¿¡æ¯ã€‚è¿™ä¸ªå‡½æ•°
    ä½¿ç”¨å¤šç§ç­–ç•¥æ¥æå–æœ€æœ‰ç”¨çš„é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©åç»­çš„é”™è¯¯ä¿®æ­£ã€‚
    
    å‚æ•°:
        simulation_output (str): æ¨¡æ‹Ÿè¿è¡Œçš„å®Œæ•´è¾“å‡ºå­—ç¬¦ä¸²
    
    è¿”å›:
        str: æå–çš„å…³é”®é”™è¯¯ä¿¡æ¯
    
    å·¥ä½œç­–ç•¥:
        1. ä¼˜å…ˆæŸ¥æ‰¾"Cleaned STDERR"éƒ¨åˆ†ï¼ˆæœ€å¹²å‡€çš„é”™è¯¯ä¿¡æ¯ï¼‰
        2. å…¶æ¬¡ï¼ŒæŸ¥æ‰¾å¹¶è¿”å›æœ€åä¸€ä¸ªTracebackçš„å®Œæ•´å†…å®¹
        3. å¦‚æœæ²¡æœ‰ï¼Œæœç´¢æ ‡å‡†é”™è¯¯å…³é”®è¯
        4. å¦‚æœä»ç„¶æ²¡æœ‰ï¼ŒæŸ¥æ‰¾è­¦å‘Šä¿¡æ¯
        5. æœ€åæä¾›é€šç”¨çš„å¤±è´¥ä¿¡æ¯
    """
    # ç­–ç•¥1: é¦–å…ˆï¼Œå°è¯•æŸ¥æ‰¾Cleaned STDERRéƒ¨åˆ†
    cleaned_stderr_match = re.search(r"--- Cleaned STDERR ---\n(.*?)(?:\n---|\Z)", simulation_output, re.DOTALL)
    if cleaned_stderr_match and cleaned_stderr_match.group(1).strip():
        # å¦‚æœæœ‰éç©ºçš„cleaned stderrï¼Œä¼˜å…ˆä½¿ç”¨å®ƒ
        # ä½†æˆ‘ä»¬ä»ç„¶è¦æ£€æŸ¥é‡Œé¢æ˜¯å¦æœ‰Traceback
        cleaned_stderr = cleaned_stderr_match.group(1).strip()
        traceback_header = "Traceback (most recent call last):"
        if traceback_header in cleaned_stderr:
             # è¿”å›åŒ…å«Tracebackçš„éƒ¨åˆ†
             return cleaned_stderr[cleaned_stderr.rfind(traceback_header):]
        # å¦‚æœæ²¡æœ‰tracebackï¼Œä½†æœ‰å†…å®¹ï¼Œä¹Ÿè¿”å›å®ƒ
        return cleaned_stderr

    # ç­–ç•¥2 (æ”¹è¿›): æŸ¥æ‰¾æœ€åä¸€ä¸ªTraceback
    traceback_header = "Traceback (most recent call last):"
    if traceback_header in simulation_output:
        # ç›´æ¥è¿”å›ä»æœ€åä¸€ä¸ªTracebackå¼€å§‹åˆ°å­—ç¬¦ä¸²æœ«å°¾çš„æ‰€æœ‰å†…å®¹
        return simulation_output[simulation_output.rfind(traceback_header):]

    # ç­–ç•¥3: åŸå§‹çš„å…³é”®è¯æœç´¢ä½œä¸ºåå¤‡
    error_lines = []
    in_traceback = False
    error_keywords = ["Error", "Exception", "Traceback (most recent call last)", "FAILED"]
    
    for line in simulation_output.splitlines():
        if any(keyword in line for keyword in error_keywords):
            in_traceback = True
        if in_traceback:
            error_lines.append(line)
            if len(error_lines) > 20 and not line.startswith(" "):
                break 
    
    if error_lines:
        return "\n".join(error_lines)

    # ç­–ç•¥4: æ£€æŸ¥è­¦å‘Š
    warning_lines = [line for line in simulation_output.splitlines() if "warning" in line.lower() or "caution" in line.lower()]
    if warning_lines:
        return "Warnings found:\n" + "\n".join(warning_lines[:10])
            
    # ç­–ç•¥5: é€šç”¨å¤±è´¥
    if "FAIL" in simulation_output.upper():
         return "\n".join(simulation_output.splitlines()[-15:])
         
    return "No specific error details extracted, but simulation did not succeed."

# ============================================================================
# LangGraphèŠ‚ç‚¹å‡½æ•°éƒ¨åˆ†
# ============================================================================

def generate_code_node(state: CodeGenerationState):
    """
    ä»£ç ç”ŸæˆèŠ‚ç‚¹å‡½æ•°
    - é¦–æ¬¡å°è¯•: ç”Ÿæˆå®Œæ•´çš„Pythonåè®®ä»£ç 
    - åç»­å°è¯•: ç”Ÿæˆä¸€ä¸ªdiffè¡¥ä¸å¹¶åº”ç”¨å®ƒæ¥ä¿®æ­£ä»£ç 
    """
    attempt_num = state['attempts'] + 1
    print(f"--- Graph: Generating Code (Attempt {attempt_num}) ---")
    reporter = state.get('iteration_reporter')
    final_code = None
    llm_diff_output = None

    # ä¼˜åŒ–ç‚¹: æ ¹æ®ç¡¬ä»¶é…ç½®åŠ¨æ€é€‰æ‹©æ­£ç¡®çš„ç¡¬ä»¶åˆ—è¡¨å’Œæç¤ºè¯
    hardware_context = state["hardware_context"]
    is_flex = "flex" in hardware_context.lower()
    
    # æ ¹æ®æœºå™¨äººç±»å‹é€‰æ‹©æ­£ç¡®çš„é…ç½®
    if is_flex:
        print("Debug - Detected 'Flex' robot. Using Flex-specific hardware lists and prompt.")
        valid_labware = LABWARE_FOR_FLEX
        valid_instruments = INSTRUMENTS_FOR_FLEX
        valid_modules = MODULES_FOR_FLEX
        code_gen_chain = code_gen_chain_flex
        code_correction_chain = code_correction_chain_flex
        common_pitfalls_str = "" # Not used for Flex
    else:
        print("Debug - Detected 'OT-2' robot (or default). Using OT-2-specific hardware lists and prompt.")
        valid_labware = LABWARE_FOR_OT2
        valid_instruments = INSTRUMENTS_FOR_OT2
        valid_modules = MODULES_FOR_OT2
        code_gen_chain = code_gen_chain_ot2
        code_correction_chain = code_correction_chain_ot2
        common_pitfalls_str = "\n".join(f"- {pitfall}" for pitfall in COMMON_PITFALLS_OT2)

    api_version_match = re.search(r"API Version:\s*([\d.]+)", hardware_context)
    api_version = api_version_match.group(1) if api_version_match else "2.19"

    valid_labware_str = "\n".join(f"- {name}" for name in valid_labware)
    valid_instruments_str = "\n".join(f"- {name}" for name in valid_instruments)
    valid_modules_str = "\n".join(f"- {name}" for name in valid_modules)

    if state['attempts'] == 0:
        # é¦–æ¬¡å°è¯•: ä»SOPç”Ÿæˆå®Œæ•´ä»£ç 
        if reporter:
            reporter({
                "event_type": "code_attempt", "attempt_num": attempt_num,
                "message": f"Generating full code from SOP (Attempt {attempt_num})"
            })
        
        # åŠ¨æ€æ„å»ºchain_inputï¼ŒåªåŒ…å«å½“å‰promptéœ€è¦çš„å˜é‡
        chain_input = {
            "hardware_context": state["hardware_context"],
            "sop_text": state['original_sop'],
            "feedback_for_llm": "", 
            "previous_code": "N/A",
            "valid_labware_list_str": valid_labware_str,
            "valid_instrument_list_str": valid_instruments_str,
            "valid_module_list_str": valid_modules_str,
            "code_examples_str": CODE_EXAMPLES,
            "apiLevel": api_version,
        }
        if not is_flex:
            chain_input["common_pitfalls_str"] = common_pitfalls_str

        raw_generated_code = code_gen_chain.run(chain_input)
        
        # å¢åŠ åå¤„ç†æ­¥éª¤æ¥æ¸…æ´—è¾“å‡º
        if "</think>" in raw_generated_code:
            raw_generated_code = raw_generated_code.split("</think>", 1)[-1]

        # æ¸…ç†Markdownä»£ç å—æ ‡è®°
        if raw_generated_code.strip().startswith("```python"):
            raw_generated_code = raw_generated_code.strip()[9:]
            if raw_generated_code.strip().endswith("```"):
                raw_generated_code = raw_generated_code.strip()[:-3]
        final_code = raw_generated_code.strip()

    else:
        # åç»­å°è¯•: ä½¿ç”¨å¢é‡ä¿®å¤ç­–ç•¥ (diff_edit)
        if reporter:
            reporter({
                "event_type": "diff_generation_start", "attempt_num": attempt_num,
                "message": f"Generating diff patch (Attempt {attempt_num})"
            })
        
        previous_code = state["python_code"]
        feedback = state["feedback_for_llm"]

        chain_input = {
            "analysis_of_failure": feedback.get("analysis", "N/A"),
            "recommended_action": feedback.get("action", "N/A"),
            "full_error_log": feedback.get("error_log", "N/A"),
            "previous_code": previous_code,
            "valid_labware_list_str": valid_labware_str,
            "valid_instrument_list_str": valid_instruments_str,
            "valid_module_list_str": valid_modules_str,
        }
        
        generated_diff = code_correction_chain.run(chain_input)
        llm_diff_output = generated_diff

        if reporter:
            reporter({
                "event_type": "diff_generated", "attempt_num": attempt_num,
                "diff_output": generated_diff, "message": "Diff patch generated, now applying..."
            })
            
        try:
            final_code = apply_diff(previous_code, generated_diff)
            if reporter:
                reporter({"event_type": "diff_applied", "attempt_num": attempt_num, "message": "Diff patch applied successfully."})
        except ValueError as e:
            print(f"CRITICAL: Failed to apply diff on attempt {attempt_num}: {e}")
            final_code = previous_code
            if reporter:
                reporter({
                    "event_type": "diff_failed", "attempt_num": attempt_num,
                    "error_details": str(e),
                    "message": "Error: Failed to apply AI-generated diff patch. This usually means the SEARCH block did not match."
                })

    if reporter:
        reporter({
            "event_type": "code_generated", "attempt_num": attempt_num,
            "generated_code": final_code, "message": f"Code generation complete. Length: {len(final_code)} chars"
        })
    
    return {
        "python_code": final_code,
        "llm_diff_output": llm_diff_output,
        "attempts": state["attempts"] + 1
    }

def simulate_code_node(state: CodeGenerationState):
    """
    ä»£ç æ¨¡æ‹ŸèŠ‚ç‚¹å‡½æ•°
    è¿è¡ŒOpentronsæ¨¡æ‹Ÿå™¨æ¥éªŒè¯ç”Ÿæˆçš„ä»£ç 
    """
    print("--- Graph: Simulating Code ---")
    
    # å‘å‰ç«¯æŠ¥å‘Šæ¨¡æ‹Ÿå¼€å§‹
    if state.get('iteration_reporter'):
        state['iteration_reporter']({
            "event_type": "simulation_start",
            "attempt_num": state['attempts'],
            "message": f"Starting simulation for attempt #{state['attempts']}"
        })
    
    # è·å–è¦æ¨¡æ‹Ÿçš„ä»£ç 
    code_to_simulate = state["python_code"]
    if not code_to_simulate:
        # å¦‚æœä»£ç ä¸ºç©ºï¼Œç›´æ¥è¿”å›é”™è¯¯
        result = {"success": False, "error_details": "Code generation resulted in empty script."}
    else:
        # è¿è¡ŒOpentronsæ¨¡æ‹Ÿå™¨
        result = run_opentrons_simulation(code_to_simulate, return_structured=True)
    
    # å‘å‰ç«¯æŠ¥å‘Šæ¨¡æ‹Ÿç»“æœ
    if state.get('iteration_reporter'):
        state['iteration_reporter']({
            "event_type": "simulation_log_raw",
            "attempt_num": state['attempts'],
            "raw_output": result.get("raw_output", ""),
            "structured_result": result,
            "message": f"Simulation complete. Status: {result.get('final_status', 'Unknown')}"
        })
    
    # è¿”å›åŒ…å«æ¨¡æ‹Ÿç»“æœçš„çŠ¶æ€æ›´æ–°
    return {"simulation_result": result}

def prepare_feedback_node(state: CodeGenerationState):
    """
    åˆ†ææ¨¡æ‹Ÿå¤±è´¥å¹¶ä¸ºLLMå‡†å¤‡ç»“æ„åŒ–çš„ã€å¯æ“ä½œçš„åé¦ˆã€‚
    """
    print("--- Graph: Preparing Intelligent Feedback for LLM ---")
    
    simulation_result = state["simulation_result"]
    raw_error_output = simulation_result.get("raw_output", "")
    error_details = extract_error_from_simulation(raw_error_output)
    
    # æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦å› ç›¸åŒé”™è¯¯é™·å…¥å¾ªç¯
    previous_feedback = state.get("feedback_for_llm", {})
    previous_error = previous_feedback.get("error_log", "")
    # å¦‚æœé”™è¯¯ä¿¡æ¯å®Œå…¨ç›¸åŒï¼Œå¹¶ä¸”å·²ç»å°è¯•è¶…è¿‡1æ¬¡ï¼Œåˆ™è®¤ä¸ºæ˜¯å¡ä½äº†
    is_stuck = error_details == previous_error and state["attempts"] > 1

    if is_stuck:
        print("ğŸ”¥ğŸ”¥ğŸ”¥ LOOP DETECTED! The previous fix failed. Escalating feedback to LLM. ğŸ”¥ğŸ”¥ğŸ”¥")
        # æä¾›ä¸€ä¸ªæ›´å¼ºçƒˆçš„æŒ‡ä»¤æ¥æ‰“ç ´å¾ªç¯
        analysis = (
            "The previous attempt to fix the code was unsuccessful and resulted in the exact same error. "
            "This indicates the initial analysis or the proposed fix was incorrect. A different approach is required."
        )
        action = (
            "Action: **STOP** and re-evaluate the problem from the beginning. "
            "Do not repeat the previous failed attempt. "
            "1. **Re-read the SOP and the code from scratch.** Look for fundamental logical errors. "
            "2. **Ignore your previous analysis.** It was wrong. "
            "3. **Propose a completely new and different solution.** If you changed a labware name before, maybe the problem is the deck slot. If you changed a parameter, maybe the entire function call is wrong. Be creative and think of an alternative fix."
        )
    else:
        # --- Start of intelligent feedback generation ---
        analysis = "An unknown error occurred."
        action = "Please re-read the SOP and your generated code carefully. Check for any logical inconsistencies or deviations from the examples provided."
        
        # 1. Labware/Instrument/Module Loading & Configuration Errors
        if "LabwareLoadError" in error_details or "cannot find a definition for labware" in error_details:
            analysis = "The simulation failed with a `LabwareLoadError`. This almost always means a labware `load_name` in your script does not exactly match a name from the `VALID LABWARE NAMES` list, or it is not compatible with the robot type."
            action = "Action: Carefully check every `protocol.load_labware()` call. Compare the `load_name` string against the provided list and correct any misspelling or inconsistency. Ensure you are using labware compatible with the specified robot."
        elif "InstrumentLoadError" in error_details or "cannot find a definition for instrument" in error_details:
            analysis = "The simulation failed with an `InstrumentLoadError`. This means a pipette `instrument_name` in your script is incorrect."
            action = "Action: Check your `protocol.load_instrument()` call. The `instrument_name` must be an exact match from the `VALID INSTRUMENT NAMES` list for the specified robot."
        elif "ModuleLoadError" in error_details:
            analysis = "The simulation failed with a `ModuleLoadError`. This means a module `load_name` in your script is incorrect."
            action = "Action: Check your `protocol.load_module()` call. The `load_name` must be an exact match from the `VALID MODULE NAMES` list."
        elif "DeckConflictError" in error_details:
            analysis = "The simulation failed with a `DeckConflictError`. This means the protocol tried to load two different items (labware or modules) into the same deck slot."
            action = "Action: Review all `protocol.load_labware()` and `protocol.load_module()` calls. Ensure that each item is assigned a unique and valid deck slot number or address."
        elif "KeyError" in error_details and re.search(r"\'[A-D][1-9][0-9]*\'", error_details):
            analysis = "The simulation failed with a `KeyError` on an alphanumeric key (e.g., 'D2', 'C3'). This is a classic OT-2 error. OT-2 protocols require deck slots to be numeric strings like '1', '2', '10'."
            action = "Action: This is an OT-2 protocol. You must change all deck slot locations from the alphanumeric format (like 'D2') to the correct numeric string format (like '2'). Review all `protocol.load_labware()` and `protocol.load_module()` calls and fix the slot names."
        elif "valid deck slot must be a string" in error_details: # Fallback for more verbose errors
            analysis = "The simulation failed because an invalid deck slot format was used. For OT-2, deck slots must be strings of numbers (e.g., '1', '2', '10'), not 'A1', 'B2', etc."
            action = "Action: Review all `protocol.load_labware()` and `protocol.load_module()` calls. Change all deck slot locations to the correct OT-2 number format (e.g., change 'A1' to '1')."
        elif "InvalidSpecificationForRobotTypeError" in error_details:
            analysis = "The simulation failed with `InvalidSpecificationForRobotTypeError`. This is a critical error indicating a mismatch between the robot type and the hardware or API version used in the script."
            action = "Action: The `requirements` or `metadata` in your script must match the `Hardware Configuration`. If the hardware says 'Flex', your script MUST use `requirements = {'robotType': 'Flex', ...}`. If it says 'OT-2', it MUST use `metadata = {'apiLevel': '...'}`. Also, ensure all loaded labware and pipettes are compatible with that robot."

        # 2. Pythonè¯­æ³•å’Œå±æ€§é”™è¯¯
        elif "SyntaxError" in error_details:
            analysis = "The script failed with a Python `SyntaxError`. This is a basic code structure error."
            action = "Action: Review the line indicated in the error for mistakes like missing commas, incorrect indentation, or unclosed parentheses. Fix the Python syntax."
        elif "AttributeError" in error_details:
            analysis = f"The script failed with an `AttributeError`. This often means you are trying to use a method or property that doesn't exist for an object (like a pipette or labware) in the specified API version. The error was: `{error_details}`"
            action = "Action: Review the Opentrons API documentation for the correct methods and parameters for liquid handling and module control. Pay close attention to the provided `DETAILED CODE EXAMPLES` which show valid patterns."
        elif "NameError" in error_details:
            analysis = f"The script failed with a `NameError`, meaning a variable or function was used before it was defined. Error: `{error_details}`"
            action = "Action: Ensure all variables (for labware, pipettes, etc.) are defined with `protocol.load_*` before you use them in commands."
            
        # 3. åè®®é€»è¾‘ä¸çŠ¶æ€é”™è¯¯
        elif "NoTrashDefinedError" in error_details:
            analysis = "The simulation failed with a `NoTrashDefinedError`. This typically happens in Flex protocols when an action requires a trash container (like dropping a tip), but one has not been defined."
            action = "Action: For Flex protocols, you must explicitly load a trash bin. Add a line like `trash = protocol.load_trash_bin('A3')` to your labware loading section. For OT-2 protocols, use `protocol.fixed_trash['A1']` instead of loading a separate trash bin."
        elif "TipAttachedError" in error_details:
            analysis = "The simulation failed with a `TipAttachedError`. This means the protocol attempted to pick up a new tip when a tip was already attached to the pipette."
            action = "Action: Ensure every `pipette.pick_up_tip()` call is preceded by a `pipette.drop_tip()` or `pipette.return_tip()` call from the previous liquid handling step. Do not call `pick_up_tip()` twice in a row."
        elif "missing tip" in error_details.lower():
            analysis = "The protocol tried to perform a liquid handling action without a tip."
            action = "Action: Ensure you call `pipette.pick_up_tip()` before any `aspirate` or `dispense` commands. Also, make sure you don't drop a tip (`pipette.drop_tip()`) and then try to use the pipette again without picking up a new one."
        elif "Cannot aspirate when module is engaged" in error_details:
            analysis = "The simulation failed because the protocol tried to aspirate liquid while the Magnetic Module was engaged. This is not allowed as it would aspirate magnetic beads."
            action = "Action: Before the aspirate command that caused the error, you must call `magnetic_module.disengage()`. This moves the magnets away from the plate, allowing the pipette to safely aspirate the supernatant."
        elif "volume" in error_details.lower() and ("out of range" in error_details.lower() or "not a valid" in error_details.lower()):
            analysis = "The protocol tried to use a volume that is outside the valid range for the specified pipette."
            action = "Action: Check all `aspirate`, `dispense`, and `mix` commands. Ensure the volumes are within the minimum and maximum capacity of the pipette being used."
        elif "Cannot perform action" in error_details and "while module is" in error_details:
            analysis = "The protocol attempted an action on a module that is currently busy or in an incompatible state (e.g., trying to open a Heater-Shaker latch while it is shaking)."
            action = "Action: Ensure you stop the module's current action before proceeding. For example, call `heater_shaker.deactivate_shaker()` before trying `heater_shaker.open_labware_latch()`."
        
        else:
            # æ”¹è¿›é€šç”¨åå¤‡æ–¹æ¡ˆ
            analysis = "The simulation failed with an error that was not automatically categorized. This could be a complex logical error in the protocol steps."
            action = "Action: Please carefully review the [Full Error Log] and the [Previous Failed Code] below. Analyze the script's logic against the SOP to identify the root cause and generate a corrected version."

        # --- End of intelligent feedback generation ---

    if state.get('iteration_reporter'):
        state['iteration_reporter']({
            "event_type": "iteration_result",
            "attempt_num": state['attempts'],
            "status": "FAILED",
            "error_details": error_details,
            "message": f"Attempt #{state['attempts']} failed."
        })
    
    feedback_dict = {
        "analysis": analysis,
        "action": action,
        "error_log": error_details,
    }
    
    return {"feedback_for_llm": feedback_dict}

def should_continue(state: CodeGenerationState):
    """
    LangGraphæ¡ä»¶è¾¹å‡½æ•°ï¼šæ ¸å¿ƒå†³ç­–å¼•æ“
    =====================================
    è¿™æ˜¯LangGraphå·¥ä½œæµçš„å…³é”®å†³ç­–ç‚¹ï¼Œæ ¹æ®æ¨¡æ‹Ÿç»“æœå’Œå°è¯•æ¬¡æ•°å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š
    - å¦‚æœæ¨¡æ‹ŸæˆåŠŸ â†’ ç»“æŸæµç¨‹ï¼Œè¿”å›æœ€ç»ˆä»£ç 
    - å¦‚æœæ¨¡æ‹Ÿå¤±è´¥ä½†æœªè¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° â†’ ç»§ç»­å¾ªç¯ï¼Œè¿›å…¥åé¦ˆå‡†å¤‡é˜¶æ®µ
    - å¦‚æœè¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° â†’ å¼ºåˆ¶ç»“æŸï¼Œè¿”å›å¤±è´¥çŠ¶æ€
    
    å†³ç­–é€»è¾‘éµå¾ª"å¼€å‘-æµ‹è¯•-è°ƒè¯•"çš„è¿­ä»£æ¨¡å¼ï¼Œæ¨¡æ‹ŸçœŸå®çš„ç¼–ç¨‹å·¥ä½œæµã€‚
    """
    print("=== [LangGraph Decision Engine] åˆ†æå½“å‰çŠ¶æ€ ===")
    
    # è·å–å…³é”®çŠ¶æ€ä¿¡æ¯
    simulation_result = state.get("simulation_result")
    current_attempt = state.get("attempts", 0)
    max_attempts = state.get("max_attempts", 5)
    
    print(f"[Decision Engine] å½“å‰å°è¯•: {current_attempt}/{max_attempts}")
    
    if not simulation_result:
        print("[Decision Engine] âš ï¸  ç¼ºå°‘æ¨¡æ‹Ÿç»“æœï¼Œç»§ç»­ä¸‹ä¸€è½®ç”Ÿæˆ")
        return "continue"
    
    # æ£€æŸ¥æˆåŠŸçŠ¶æ€å’Œè­¦å‘Š
    success = simulation_result.get("success", False)
    has_warnings = simulation_result.get("has_warnings", False)
    error_details = simulation_result.get("error_details", "")
    
    print(f"[Decision Engine] æ¨¡æ‹Ÿç»“æœ: æˆåŠŸ={success}, æœ‰è­¦å‘Š={has_warnings}")
    
    if success and not has_warnings:
        # âœ… ç†æƒ³æƒ…å†µï¼šä»£ç å®Œç¾è¿è¡Œï¼Œæ— ä»»ä½•é—®é¢˜
        print("[Decision Engine] âœ… æ¨¡æ‹Ÿå®Œå…¨æˆåŠŸï¼æµç¨‹ç»“æŸ")
        if state.get('iteration_reporter'):
            state['iteration_reporter']({
                "event_type": "iteration_result",
                "attempt_num": current_attempt,
                "status": "SUCCESS",
                "final_code": state.get("python_code", ""),
                "message": f"ç¬¬ {current_attempt} æ¬¡å°è¯•æˆåŠŸï¼æ¨¡æ‹Ÿé€šè¿‡ï¼Œæ— è­¦å‘Šã€‚"
            })
        return "end"
    elif success and has_warnings:
        # âš ï¸  å¯æ¥å—æƒ…å†µï¼šä»£ç èƒ½è¿è¡Œï¼Œä½†æœ‰è­¦å‘Šï¼ˆå¦‚å¼ƒç”¨æé†’ç­‰ï¼‰
        print("[Decision Engine] âš ï¸  æ¨¡æ‹ŸæˆåŠŸä½†æœ‰è­¦å‘Šï¼Œä»è§†ä¸ºå®Œæˆ")
        if state.get('iteration_reporter'):
            state['iteration_reporter']({
                "event_type": "iteration_result",
                "attempt_num": current_attempt,
                "status": "SUCCESS_WITH_WARNINGS",
                "warning_details": error_details,
                "message": f"ç¬¬ {current_attempt} æ¬¡å°è¯•æˆåŠŸï¼Œä½†å­˜åœ¨è­¦å‘Šã€‚"
            })
        return "end"
    elif current_attempt >= max_attempts:
        # ğŸ’€ å¤±è´¥æƒ…å†µï¼šå·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œå¿…é¡»åœæ­¢é¿å…æ— é™å¾ªç¯
        print(f"[Decision Engine] ğŸ’€ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_attempts})ï¼Œå¼ºåˆ¶ç»“æŸ")
        if state.get('iteration_reporter'):
            state['iteration_reporter']({
                "event_type": "iteration_result",
                "attempt_num": current_attempt,
                "status": "FINAL_FAILED",
                "error_details": error_details,
                "final_code": state.get("python_code", ""),
                "message": f"æœ€ç»ˆå¤±è´¥ï¼š{max_attempts} æ¬¡å°è¯•åä»æ— æ³•é€šè¿‡æ¨¡æ‹Ÿã€‚"
            })
        return "end"
    else:
        # ğŸ”„ ç»§ç»­æƒ…å†µï¼šæ¨¡æ‹Ÿå¤±è´¥ï¼Œä½†è¿˜æœ‰é‡è¯•æœºä¼šï¼Œè¿›å…¥è°ƒè¯•ä¿®å¤æµç¨‹
        print(f"[Decision Engine] ğŸ”„ æ¨¡æ‹Ÿå¤±è´¥ï¼Œå‡†å¤‡ç¬¬ {current_attempt + 1} æ¬¡å°è¯•")
        print(f"[Decision Engine] é”™è¯¯ä¿¡æ¯: {error_details[:100]}..." if error_details else "æ— å…·ä½“é”™è¯¯è¯¦æƒ…")
        return "continue"

# ============================================================================
# LangGraphå·¥ä½œæµæ„å»ºå’Œç¼–è¯‘
# ============================================================================

# åˆ›å»ºå’Œç¼–è¯‘LangGraphå·¥ä½œæµ
workflow = StateGraph(CodeGenerationState)

# å‘å›¾ä¸­æ·»åŠ èŠ‚ç‚¹
workflow.add_node("generator", generate_code_node)           # ä»£ç ç”Ÿæˆå™¨èŠ‚ç‚¹
workflow.add_node("simulator", simulate_code_node)           # ä»£ç æ¨¡æ‹Ÿå™¨èŠ‚ç‚¹
workflow.add_node("feedback_preparer", prepare_feedback_node) # åé¦ˆå‡†å¤‡å™¨èŠ‚ç‚¹

# å®šä¹‰å›¾çš„æµç¨‹
workflow.add_edge(START, "generator")                        # ä»å¼€å§‹èŠ‚ç‚¹åˆ°ä»£ç ç”Ÿæˆå™¨
workflow.add_edge("generator", "simulator")                  # ä»ä»£ç ç”Ÿæˆå™¨åˆ°æ¨¡æ‹Ÿå™¨
workflow.add_conditional_edges(                              # æ¡ä»¶è¾¹ï¼šæ ¹æ®æ¨¡æ‹Ÿç»“æœå†³å®šä¸‹ä¸€æ­¥
    "simulator",
    should_continue,
    {
        "continue": "feedback_preparer",  # å¦‚æœéœ€è¦ç»§ç»­ï¼Œå»åé¦ˆå‡†å¤‡å™¨
        "end": END                        # å¦‚æœå®Œæˆï¼Œç»“æŸæµç¨‹
    }
)
workflow.add_edge("feedback_preparer", "generator")          # å¾ªç¯å›åˆ°ä»£ç ç”Ÿæˆå™¨

# å°†å›¾ç¼–è¯‘ä¸ºå¯è¿è¡Œçš„åº”ç”¨ç¨‹åº
code_generation_graph = workflow.compile()

def run_code_generation_graph(
    tool_input: str, 
    max_iterations: int,
    iteration_reporter: Optional[Callable[[Dict[str, Any]], None]] = None
) -> str:
    """
    åŸºäºLangGraphçš„æ–°ä»£ç ç”Ÿæˆå‡½æ•°ï¼Œæ›¿ä»£æ—§çš„è¿­ä»£å¾ªç¯
    
    å‚æ•°:
        tool_input: åŒ…å«SOPå’Œç¡¬ä»¶é…ç½®çš„è¾“å…¥å­—ç¬¦ä¸²ï¼Œç”¨ç‰¹å®šåˆ†éš”ç¬¦åˆ†éš”
        max_iterations: è¦†ç›–é»˜è®¤çš„æœ€å¤§å°è¯•æ¬¡æ•°
        iteration_reporter: å¯é€‰çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºæŠ¥å‘Šè¿›åº¦ä¿¡æ¯
    
    è¿”å›:
        str: ç”Ÿæˆçš„åè®®ä»£ç æˆ–é”™è¯¯ä¿¡æ¯
    """
    try:
        print(f"Debug - [langchain_agent.py] Entering LangGraph-based code generation")

        # ä¸ºå‘½ä»¤è¡Œå…¼å®¹æ€§å®šä¹‰é»˜è®¤æŠ¥å‘Šå™¨
        def default_reporter(event_data: Dict[str, Any]):
            if event_data["event_type"] == "iteration_log":
                print(f"[ProtocolCodeGenerator] {event_data['message']}")
            elif event_data["event_type"] == "code_attempt":
                print(f"[ProtocolCodeGenerator] ç”Ÿæˆä»£ç å°è¯• {event_data['attempt_num']}")
            elif event_data["event_type"] == "simulation_start":
                print(f"[ProtocolCodeGenerator] å¼€å§‹æ¨¡æ‹ŸéªŒè¯ç¬¬ {event_data['attempt_num']} æ¬¡å°è¯•...")
            elif event_data["event_type"] == "simulation_log_raw":
                print(f"[ProtocolCodeGenerator] æ¨¡æ‹Ÿç»“æœ: {event_data.get('message', '')}")
            elif event_data["event_type"] == "iteration_result":
                print(f"[ProtocolCodeGenerator] ç¬¬ {event_data['attempt_num']} æ¬¡å°è¯•ç»“æœ: {event_data['status']}")

        reporter = iteration_reporter or default_reporter

        # è§£æå·¥å…·è¾“å…¥
        separator = "\n---CONFIG_SEPARATOR---\n"
        if separator not in tool_input:
            error_msg = "Error: Input for ProtocolCodeGenerator must contain SOP and hardware context separated by '\n---CONFIG_SEPARATOR---\n'."
            reporter({
                "event_type": "iteration_log",
                "message": error_msg,
                "attempt_num": 0,
                "max_attempts": 5
            })
            return error_msg
        
        # åˆ†ç¦»SOPå’Œç¡¬ä»¶é…ç½®
        original_sop, hardware_context = tool_input.split(separator, 1)
        original_sop = original_sop.strip()
        hardware_context = hardware_context.strip()

        # ä»ç¡¬ä»¶é…ç½®ä¸­æå–APIç‰ˆæœ¬
        api_version_match = re.search(r"API Version:\s*([\d.]+)", hardware_context)
        api_version_for_prompt = api_version_match.group(1) if api_version_match else "2.19"
        
        # è®¾ç½®åˆå§‹çŠ¶æ€ï¼Œä½¿ç”¨è¦†ç›–å€¼
        initial_state = CodeGenerationState(
            original_sop=original_sop,
            hardware_context=hardware_context,
            python_code=None,
            llm_diff_output=None,
            simulation_result=None,
            feedback_for_llm={},
            attempts=0,
            max_attempts=max_iterations,
            iteration_reporter=reporter
        )
        
        # æ¯æ¬¡å°è¯•æ¶‰åŠ3ä¸ªèŠ‚ç‚¹ï¼ˆgenerator -> simulator -> feedback_preparerï¼‰
        # æ‰€ä»¥9æ¬¡å°è¯• Ã— 3ä¸ªèŠ‚ç‚¹ =27æ¬¡æ€»èŠ‚ç‚¹è®¿é—®
        config = {"recursion_limit": 50}
        final_state = code_generation_graph.invoke(initial_state, config=config)
        
        # æ ¼å¼åŒ–å¹¶è¿”å›æœ€ç»ˆç»“æœ
        simulation_result = final_state.get("simulation_result", {})
        success = simulation_result.get("success", False)
        has_warnings = simulation_result.get("has_warnings", False)
        
        if success and not has_warnings:
            # æˆåŠŸä¸”æ— è­¦å‘Šï¼Œç›´æ¥è¿”å›ä»£ç 
            return final_state.get("python_code", "")
        elif success and has_warnings:
            # æˆåŠŸä½†æœ‰è­¦å‘Šï¼Œè¿”å›è­¦å‘Šä¿¡æ¯å’Œä»£ç 
            warning_details = simulation_result.get("error_details", "")
            final_result = f"Warning: Protocol simulation succeeded with warnings. Please review these warnings before using:\n{warning_details}\n\nGenerated Code:\n```python\n{final_state.get('python_code', '')}\n```"
            return final_result
        else:
            # å¤±è´¥ï¼Œè¿”å›ç»“æ„åŒ–çš„é”™è¯¯ä¿¡æ¯
            error_details = simulation_result.get('error_details', 'Unknown failure')
            last_code = final_state.get('python_code', '')
            
            # æ„å»ºç”¨æˆ·å‹å¥½çš„é”™è¯¯æŠ¥å‘Š
            final_error = f"""**åè®®ç”Ÿæˆå¤±è´¥æŠ¥å‘Š**

**æ€»ä½“çŠ¶æ€**: ç»è¿‡ {final_state['attempts']} æ¬¡å°è¯•åå¤±è´¥

**æœ€åä¸€æ¬¡é”™è¯¯è¯¦æƒ…**:
{error_details}

**æœ€åç”Ÿæˆçš„ä»£ç ** (å¯å‚è€ƒä¿®æ”¹):
```python
{last_code}
```

**åŸå§‹SOP**:
{original_sop}

**å»ºè®®**:
- æ£€æŸ¥SOPä¸­æ˜¯å¦åŒ…å«ä¸å…¼å®¹çš„ç¡¬ä»¶è¦æ±‚
- ç¡®è®¤è¯•å‰‚ä½“ç§¯å’Œç§»æ¶²å™¨å®¹é‡åŒ¹é…
- éªŒè¯deck layoutæ˜¯å¦æ­£ç¡®é…ç½®
- å¦‚æœé”™è¯¯æŒç»­ï¼Œè¯·è€ƒè™‘ç®€åŒ–å®éªŒæ­¥éª¤"""
            return final_error

    except Exception as e:
        # å¼‚å¸¸å¤„ç†
        print(f"Debug - [langchain_agent.py] Exception in LangGraph code generation: {e}")
        import traceback
        error_traceback = traceback.format_exc()
        print(f"Debug - å®Œæ•´é”™è¯¯å †æ ˆ: {error_traceback}")
        return f"Error: An unexpected error occurred during protocol generation. Details: {str(e)}\nTraceback:\n{error_traceback}"

# ============================================================================
# NOTE: Removed deprecated wrapper function run_automated_protocol_generation_with_iteration
# Use run_code_generation_graph or run_code_generation_graph_stream directly instead
# ============================================================================

# ============================================================================
# æ–°å¢ï¼šå¼‚æ­¥æµå¼ä»£ç ç”Ÿæˆå‡½æ•°
# ============================================================================

async def run_code_generation_graph_stream(
    tool_input: str, 
    max_iterations: int
):
    """
    åŸºäºLangGraphçš„å¼‚æ­¥æµå¼ä»£ç ç”Ÿæˆå‡½æ•°
    
    æ­¤å‡½æ•°ä½¿ç”¨ LangGraph çš„ astream() æ–¹æ³•æ¥å®ç°çœŸæ­£çš„å¼‚æ­¥æµå¼å“åº”ã€‚
    å®ƒä¼šåœ¨å›¾æ‰§è¡Œçš„æ¯ä¸ªå…³é”®æ­¥éª¤å yield ä¸€ä¸ªJSONå¯¹è±¡ï¼Œå…è®¸å‰ç«¯å®æ—¶æ˜¾ç¤ºè¿›åº¦ã€‚
    
    å‚æ•°:
        tool_input: åŒ…å«SOPå’Œç¡¬ä»¶é…ç½®çš„è¾“å…¥å­—ç¬¦ä¸²ï¼Œç”¨ç‰¹å®šåˆ†éš”ç¬¦åˆ†éš”
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    ç”Ÿæˆå™¨è¿”å›:
        Dict[str, Any]: æ¯æ¬¡yieldä¸€ä¸ªåŒ…å«äº‹ä»¶ç±»å‹å’Œç›¸å…³æ•°æ®çš„JSONå¯¹è±¡
    
    äº‹ä»¶ç±»å‹è¯´æ˜:
        - "start": å¼€å§‹æ‰§è¡Œ
        - "node_start": èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
        - "node_complete": èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ
        - "attempt_result": å°è¯•ç»“æœï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
        - "final_result": æœ€ç»ˆç»“æœ
        - "error": æ‰§è¡Œé”™è¯¯
    """
    try:
        print(f"Debug - [run_code_generation_graph_stream] å¼€å§‹å¼‚æ­¥æµå¼ä»£ç ç”Ÿæˆ")
        
        # å‘é€å¼€å§‹äº‹ä»¶
        yield {
            "event_type": "start",
            "message": "å¼€å§‹åè®®ä»£ç ç”Ÿæˆæµç¨‹...",
            "timestamp": datetime.now().isoformat()
        }

        # è§£æå·¥å…·è¾“å…¥
        separator = "\n---CONFIG_SEPARATOR---\n"
        if separator not in tool_input:
            error_msg = "Error: Input for ProtocolCodeGenerator must contain SOP and hardware context separated by '\n---CONFIG_SEPARATOR---\n'."
            yield {
                "event_type": "error",
                "message": error_msg,
                "timestamp": datetime.now().isoformat()
            }
            return
        
        # åˆ†ç¦»SOPå’Œç¡¬ä»¶é…ç½®
        original_sop, hardware_context = tool_input.split(separator, 1)
        original_sop = original_sop.strip()
        hardware_context = hardware_context.strip()

        # ä»ç¡¬ä»¶é…ç½®ä¸­æå–APIç‰ˆæœ¬
        api_version_match = re.search(r"API Version:\s*([\d.]+)", hardware_context)
        api_version_for_prompt = api_version_match.group(1) if api_version_match else "2.19"
        
        # è®¾ç½®åˆå§‹çŠ¶æ€
        initial_state = CodeGenerationState(
            original_sop=original_sop,
            hardware_context=hardware_context,
            python_code=None,
            llm_diff_output=None,
            simulation_result=None,
            feedback_for_llm={},
            attempts=0,
            max_attempts=max_iterations,
            iteration_reporter=None  # ä¸éœ€è¦åœ¨æµå¼ç‰ˆæœ¬ä¸­ä½¿ç”¨å›è°ƒ
        )
        
        yield {
            "event_type": "initialization",
            "message": f"åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å°è¯•æ¬¡æ•°: {max_iterations}",
            "max_attempts": max_iterations,
            "timestamp": datetime.now().isoformat()
        }
        
        # ä½¿ç”¨ astream å¼‚æ­¥æ‰§è¡Œå›¾
        config = {"recursion_limit": 50}
        
        # è·Ÿè¸ªå½“å‰çŠ¶æ€
        current_state = initial_state
        current_attempt = 0
        
        async for chunk in code_generation_graph.astream(initial_state, config=config):
            # chunk æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯èŠ‚ç‚¹åï¼Œå€¼æ˜¯è¯¥èŠ‚ç‚¹çš„è¾“å‡º
            for node_name, node_output in chunk.items():
                print(f"Debug - [stream] Node '{node_name}' completed with output keys: {list(node_output.keys())}")
                
                # æ›´æ–°å½“å‰çŠ¶æ€
                current_state.update(node_output)
                
                if node_name == "generator":
                    # ä»£ç ç”ŸæˆèŠ‚ç‚¹å®Œæˆ
                    current_attempt = current_state.get("attempts", 0)
                    yield {
                        "event_type": "node_complete",
                        "node_name": "generator",
                        "message": f"ç¬¬ {current_attempt} æ¬¡ä»£ç ç”Ÿæˆå®Œæˆ",
                        "attempt_num": current_attempt,
                        "has_code": bool(current_state.get("python_code")),
                        "timestamp": datetime.now().isoformat()
                    }
                    
                elif node_name == "simulator":
                    # æ¨¡æ‹Ÿå™¨èŠ‚ç‚¹å®Œæˆ
                    sim_result = current_state.get("simulation_result", {})
                    success = sim_result.get("success", False)
                    has_warnings = sim_result.get("has_warnings", False)
                    
                    yield {
                        "event_type": "node_complete",
                        "node_name": "simulator",
                        "message": f"ç¬¬ {current_attempt} æ¬¡æ¨¡æ‹ŸéªŒè¯å®Œæˆ",
                        "attempt_num": current_attempt,
                        "simulation_success": success,
                        "has_warnings": has_warnings,
                        "error_details": sim_result.get("error_details", "") if not success else "",
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    # å¦‚æœæ¨¡æ‹ŸæˆåŠŸï¼Œè¿™å¯èƒ½æ˜¯æœ€ç»ˆç»“æœ
                    if success:
                        yield {
                            "event_type": "attempt_result", 
                            "status": "SUCCESS_WITH_WARNINGS" if has_warnings else "SUCCESS",
                            "attempt_num": current_attempt,
                            "message": f"ç¬¬ {current_attempt} æ¬¡å°è¯•æˆåŠŸï¼" + (" (æœ‰è­¦å‘Š)" if has_warnings else ""),
                            "final_code": current_state.get("python_code", ""),
                            "warning_details": sim_result.get("error_details", "") if has_warnings else "",
                            "timestamp": datetime.now().isoformat()
                        }
                    
                elif node_name == "feedback_preparer":
                    # åé¦ˆå‡†å¤‡å™¨èŠ‚ç‚¹å®Œæˆ
                    feedback = current_state.get("feedback_for_llm", {})
                    yield {
                        "event_type": "node_complete",
                        "node_name": "feedback_preparer",
                        "message": f"ç¬¬ {current_attempt} æ¬¡é”™è¯¯åˆ†æå®Œæˆï¼Œå‡†å¤‡ä¸‹ä¸€è½®ä¿®æ­£",
                        "attempt_num": current_attempt,
                        "has_feedback": bool(feedback),
                        "error_analysis": feedback.get("analysis", ""),
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°
                    if current_attempt >= max_iterations:
                        yield {
                            "event_type": "attempt_result",
                            "status": "FINAL_FAILED",
                            "attempt_num": current_attempt,
                            "message": f"è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_iterations})ï¼Œä»£ç ç”Ÿæˆå¤±è´¥",
                            "final_code": current_state.get("python_code", ""),
                            "error_details": sim_result.get("error_details", ""),
                            "timestamp": datetime.now().isoformat()
                        }
        
        # å‘é€æœ€ç»ˆç»“æœ
        final_simulation = current_state.get("simulation_result", {})
        final_success = final_simulation.get("success", False)
        final_warnings = final_simulation.get("has_warnings", False)
        final_code = current_state.get("python_code", "")
        
        if final_success:
            yield {
                "event_type": "final_result",
                "status": "success",
                "message": "åè®®ä»£ç ç”ŸæˆæˆåŠŸå®Œæˆï¼",
                "generated_code": final_code,
                "has_warnings": final_warnings,
                "warning_details": final_simulation.get("error_details", "") if final_warnings else "",
                "total_attempts": current_attempt,
                "timestamp": datetime.now().isoformat()
            }
        else:
            # æ„å»ºå¤±è´¥æŠ¥å‘Š
            error_details = final_simulation.get('error_details', 'Unknown failure')
            final_error = f"""**åè®®ç”Ÿæˆå¤±è´¥æŠ¥å‘Š**

**æ€»ä½“çŠ¶æ€**: ç»è¿‡ {current_attempt} æ¬¡å°è¯•åå¤±è´¥

**æœ€åä¸€æ¬¡é”™è¯¯è¯¦æƒ…**:
{error_details}

**æœ€åç”Ÿæˆçš„ä»£ç ** (å¯å‚è€ƒä¿®æ”¹):
```python
{final_code}
```

**åŸå§‹SOP**:
{original_sop}

**å»ºè®®**:
- æ£€æŸ¥SOPä¸­æ˜¯å¦åŒ…å«ä¸å…¼å®¹çš„ç¡¬ä»¶è¦æ±‚
- ç¡®è®¤è¯•å‰‚ä½“ç§¯å’Œç§»æ¶²å™¨å®¹é‡åŒ¹é…
- éªŒè¯deck layoutæ˜¯å¦æ­£ç¡®é…ç½®
- å¦‚æœé”™è¯¯æŒç»­ï¼Œè¯·è€ƒè™‘ç®€åŒ–å®éªŒæ­¥éª¤"""

            yield {
                "event_type": "final_result",
                "status": "failure",
                "message": "åè®®ä»£ç ç”Ÿæˆå¤±è´¥",
                "error_report": final_error,
                "generated_code": final_code,
                "error_details": error_details,
                "total_attempts": current_attempt,
                "timestamp": datetime.now().isoformat()
            }

    except Exception as e:
        # å¼‚å¸¸å¤„ç†
        print(f"Debug - [run_code_generation_graph_stream] Exception: {e}")
        import traceback
        error_traceback = traceback.format_exc()
        print(f"Debug - å®Œæ•´é”™è¯¯å †æ ˆ: {error_traceback}")
        
        yield {
            "event_type": "error",
            "message": f"åè®®ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}",
            "error_traceback": error_traceback,
            "timestamp": datetime.now().isoformat()
        }

# ============================================================================
# åè®®ä»£ç ç”Ÿæˆçš„ç®€åŒ–æ¥å£å‡½æ•°
# ============================================================================

def generate_protocol_code(sop_markdown: str, hardware_config: str, max_iterations: int = 5) -> str:
    """
    ç”Ÿæˆåè®®ä»£ç çš„ç®€åŒ–æ¥å£å‡½æ•°
    è¿™ä¸ªå‡½æ•°æä¾›äº†ä¸€ä¸ªæ›´ç®€å•çš„APIæ¥ç”Ÿæˆåè®®ä»£ç ï¼Œç›´æ¥è¿”å›ç»“æœ
    
    å‚æ•°:
        sop_markdown: SOPçš„markdownæ ¼å¼æ–‡æœ¬
        hardware_config: ç¡¬ä»¶é…ç½®å­—ç¬¦ä¸²
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    è¿”å›:
        str: ç”Ÿæˆçš„åè®®ä»£ç æˆ–é”™è¯¯ä¿¡æ¯
    """
    print(f"Debug - [generate_protocol_code] å¼€å§‹åè®®ä»£ç ç”Ÿæˆ (max_iterations={max_iterations})")
    
    try:
        # æ ¼å¼åŒ–è¾“å…¥å‚æ•°ï¼Œä½¿ç”¨ç‰¹å®šåˆ†éš”ç¬¦è¿æ¥SOPå’Œç¡¬ä»¶é…ç½®
        tool_input = f"{sop_markdown}\n---CONFIG_SEPARATOR---\n{hardware_config}"
        
        # è°ƒç”¨LangGraphå·¥ä½œæµç”Ÿæˆä»£ç 
        result = run_code_generation_graph(tool_input, max_iterations=max_iterations)
        
        print(f"Debug - [generate_protocol_code] ä»£ç ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(result)} å­—ç¬¦")
        return result
        
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
        print(f"Error - [generate_protocol_code] ä»£ç ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")
        return f"Error: åè®®ä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}"

# ============================================================================
# ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•
# ============================================================================

if __name__ == '__main__':
    print("Langchain agent setup complete with LangGraph-based iterative protocol generator.")
    
    # æµ‹è¯•æ–°çš„LangGraphå®ç°
    # è¿™é‡Œå®šä¹‰äº†ä¸€ä¸ªç®€å•çš„æµ‹è¯•ç”¨ä¾‹
    test_sop = "1. Add 50ul of water from Trough A1 to wells A1-A3 of a 96-well plate.\n2. Add 50ul of reagent X from Tube B1 to wells A1-A3 of the 96-well plate."
    test_hw = "Robot Model: Opentrons Flex\nAPI Version: 2.19\nLeft Pipette: flex_1channel_1000\nRight Pipette: None\nDeck Layout:\n  A1: opentrons_96_tiprack_1000ul\n  B1: opentrons_24_tuberack_eppendorf_2ml_safelock_snapcap \n  C1: corning_96_wellplate_360ul_flat"
    
    # æ ¼å¼åŒ–æµ‹è¯•è¾“å…¥å¹¶è¿è¡Œæµ‹è¯•
    test_tool_input = f"{test_sop}\n---CONFIG_SEPARATOR---\n{test_hw}"
    # æµ‹è¯•ä»£ç ç”Ÿæˆï¼ˆä½¿ç”¨å”¯ä¸€çš„å¢é‡ä¿®å¤ç­–ç•¥ï¼‰
    print("\n--- Testing code generation with diff_edit strategy ---")
    result = run_code_generation_graph(test_tool_input, max_iterations=5)
    print("\n--- LangGraph Code Generation Test Result ---")
    print(result)

# ############################################################################
# # ç¬¬ä¸€é˜¶æ®µï¼šå®šä¹‰ Agent çš„æ ¸å¿ƒå·¥å…· (è‡ªä¸»ä»£ç ç¼–è¾‘)
# ############################################################################

from langchain_core.tools import tool
from langchain.tools import BaseTool
from langgraph.types import Command

def _extract_diff_content(diff_response: str) -> str:
    """
    (å†…éƒ¨å‡½æ•°) ä» LLM ç”Ÿæˆçš„å®Œæ•´å“åº”ä¸­æå– diff å†…å®¹ã€‚
    
    LLM çš„å“åº”å¯èƒ½åŒ…å«æ€è€ƒè¿‡ç¨‹æˆ– Markdown ä»£ç å—ã€‚è¿™ä¸ªå‡½æ•°
    è´Ÿè´£æå–å‡ºå¯ä¾› `apply_diff` ä½¿ç”¨çš„çº¯ç²¹çš„ diff æ–‡æœ¬ã€‚
    
    Args:
        diff_response: LLM è¿”å›çš„åŸå§‹å­—ç¬¦ä¸²
        
    Returns:
        æå–å‡ºçš„ diff æ–‡æœ¬
    """
    # æŸ¥æ‰¾è¢« ` ```diff ` å’Œ ` ``` ` åŒ…å›´çš„ä»£ç å—
    diff_match = re.search(r'```diff\s*(.*?)\s*```', diff_response, re.DOTALL)
    if diff_match:
        # å¦‚æœæ‰¾åˆ°ï¼Œè¿”å›ä»£ç å—çš„å†…å®¹
        return diff_match.group(1).strip()
    
    # ä½œä¸ºåå¤‡æ–¹æ¡ˆï¼Œå¦‚æœæ‰¾ä¸åˆ° `diff` æ ‡è®°ï¼Œä½†å†…å®¹çœ‹èµ·æ¥åƒä¸€ä¸ª diff
    # ï¼ˆåŒ…å« SEARCH/REPLACE å—ï¼‰ï¼Œåˆ™ç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬
    if "------- SEARCH" in diff_response and "------- REPLACE" in diff_response:
        return diff_response.strip()
        
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå¯èƒ½ LLM è¿”å›äº†é diff å†…å®¹ï¼Œè¿™æ˜¯ä¸€ç§é”™è¯¯æƒ…å†µ
    # ä½†ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬è¿”å›åŸå§‹å“åº”ï¼Œè®© apply_diff æ¥å¤„ç†
    return diff_response.strip()

@tool
def modify_code_tool(original_code: str, user_instruction: str) -> str:
    """
    ä¿®æ”¹ä»£ç å·¥å…·ï¼šä¸“æ³¨äºæ ¹æ®ç”¨æˆ·æŒ‡ä»¤ä¿®æ”¹ä»£ç ï¼Œè¿”å›ä¿®æ”¹åçš„å®Œæ•´ä»£ç ã€‚
    
    æ­¤å·¥å…·ä¼šè°ƒç”¨ LLM ç”Ÿæˆ diff è¡¥ä¸ï¼Œç„¶ååº”ç”¨åˆ°åŸå§‹ä»£ç ä¸Šã€‚
    å®ƒä¸å…³å¿ƒä»£ç æ˜¯å¦èƒ½é€šè¿‡æ¨¡æ‹Ÿï¼Œåªè´Ÿè´£ä¿®æ”¹ã€‚
    
    Args:
        original_code: åŸå§‹ä»£ç å­—ç¬¦ä¸²
        user_instruction: ç”¨æˆ·ä¿®æ”¹æŒ‡ä»¤
        
    Returns:
        ä¿®æ”¹åçš„å®Œæ•´ä»£ç å­—ç¬¦ä¸²
        
    Raises:
        Exception: å¦‚æœä¿®æ”¹å¤±è´¥
    """
    try:
        print(f"Debug - [modify_code_tool] å¼€å§‹ä»£ç ä¿®æ”¹")
        
        # ä½¿ç”¨ç®€åŒ–çš„ Planner-Differ æ¶æ„
        planner_prompt = CODE_PLANNER_PROMPT_TEMPLATE.format(
            user_instruction=user_instruction,
            original_code=original_code,
            hardware_context="No specific hardware context - inferred from code",
            valid_labware_list_str="N/A - Context will be inferred from code",
            valid_instrument_list_str="N/A - Context will be inferred from code", 
            valid_module_list_str="N/A - Context will be inferred from code",
            common_pitfalls_str="- Check API compatibility (OT-2 vs Flex)\n- OT-2 uses numeric deck slots\n- Flex uses alphanumeric deck slots"
        )
        
        # æ­¥éª¤1ï¼šç”Ÿæˆä¿®æ”¹è®¡åˆ’
        planner_response = llm.invoke(planner_prompt).content.strip()
        
        # æå–JSONæ ¼å¼çš„ä¿®æ”¹è®¡åˆ’
        try:
            json_match = re.search(r'```json\s*(.*?)\s*```', planner_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = planner_response
            
            modification_plan = json.loads(json_str)
        except json.JSONDecodeError as e:
            raise Exception(f"ä¿®æ”¹è®¡åˆ’ç”Ÿæˆæ ¼å¼é”™è¯¯: {e}")
        
        # æ­¥éª¤2ï¼šç”Ÿæˆ diff
        differ_prompt = CODE_DIFFER_PROMPT_TEMPLATE.format(
            modification_plan=json.dumps(modification_plan, indent=2, ensure_ascii=False),
            original_code=original_code
        )
        
        diff_response = code_gen_llm.invoke(differ_prompt).content.strip()
        
        # æ­¥éª¤3ï¼šåº”ç”¨ diff (å¢åŠ é‡è¯•é€»è¾‘)
        diff_content = _extract_diff_content(diff_response)
        
        try:
            modified_code = apply_diff(original_code, diff_content)
        except ValueError as e:
            print(f"Warning - [modify_code_tool] Diffåº”ç”¨å¤±è´¥ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤: {e}")
            
            # å°è¯•ä¿®å¤ diff
            fixer_prompt = CODE_DIFFER_FIX_PROMPT_TEMPLATE.format(
                original_code=original_code,
                user_instruction=user_instruction,
                failed_diff=diff_content,
                error_message=str(e)
            )
            
            fixed_diff_response = code_gen_llm.invoke(fixer_prompt).content.strip()
            fixed_diff_content = _extract_diff_content(fixed_diff_response)
            
            # å†æ¬¡å°è¯•åº”ç”¨ä¿®å¤åçš„ diff
            modified_code = apply_diff(original_code, fixed_diff_content) # å¦‚æœå†æ¬¡å¤±è´¥ï¼Œä¼šè‡ªç„¶æŠ›å‡ºå¼‚å¸¸
        
        # æ­¥éª¤4ï¼šå¿«é€Ÿè¯­æ³•éªŒè¯
        syntax_valid, syntax_error = _validate_python_syntax(modified_code)
        if not syntax_valid:
            raise Exception(f"ç”Ÿæˆçš„ä»£ç è¯­æ³•é”™è¯¯: {syntax_error}")
        
        print(f"Debug - [modify_code_tool] ä»£ç ä¿®æ”¹æˆåŠŸ")
        return modified_code
        
    except Exception as e:
        print(f"Error - [modify_code_tool] ä»£ç ä¿®æ”¹å¤±è´¥: {e}")
        raise Exception(f"ä»£ç ä¿®æ”¹å¤±è´¥: {str(e)}")


@tool
def simulate_protocol_tool(code_to_simulate: str) -> str:
    """
    æ¨¡æ‹Ÿåè®®å·¥å…·ï¼šä¸“æ³¨äºéªŒè¯ä»£ç ï¼Œè¿è¡Œ Opentrons æ¨¡æ‹Ÿå™¨å¹¶è¿”å›ç»“æœã€‚
    
    æ­¤å·¥å…·æ¥æ”¶ä»£ç å¹¶è¿”å›æ¨¡æ‹Ÿå™¨è¾“å‡ºçš„å­—ç¬¦ä¸²ï¼ˆæˆåŠŸæˆ–å¤±è´¥æ—¥å¿—ï¼‰ã€‚
    
    Args:
        code_to_simulate: è¦æ¨¡æ‹Ÿçš„ä»£ç å­—ç¬¦ä¸²
        
    Returns:
        æ¨¡æ‹Ÿç»“æœçš„å­—ç¬¦ä¸²æè¿°
    """
    try:
        print(f"Debug - [simulate_protocol_tool] å¼€å§‹åè®®æ¨¡æ‹Ÿ")
        
        # è¿è¡Œ Opentrons æ¨¡æ‹Ÿå™¨
        simulation_result = run_opentrons_simulation(code_to_simulate, return_structured=True)
        
        if simulation_result["success"]:
            if simulation_result.get("has_warnings", False):
                result_msg = f"âœ… æ¨¡æ‹ŸæˆåŠŸï¼Œä½†æœ‰è­¦å‘Š:\n{simulation_result.get('warning_details', '')}"
            else:
                result_msg = "âœ… æ¨¡æ‹ŸæˆåŠŸï¼Œä»£ç éªŒè¯é€šè¿‡"
        else:
            error_details = simulation_result.get("error_details", "æœªçŸ¥é”™è¯¯")
            result_msg = f"âŒ æ¨¡æ‹Ÿå¤±è´¥:\n{error_details}"
            
            # æ·»åŠ é”™è¯¯å»ºè®®
            recommendations = simulation_result.get("recommendations", [])
            if recommendations:
                result_msg += f"\n\nå»ºè®®:\n" + "\n".join(f"- {rec}" for rec in recommendations)
        
        print(f"Debug - [simulate_protocol_tool] æ¨¡æ‹Ÿå®Œæˆ")
        return result_msg
        
    except Exception as e:
        print(f"Error - [simulate_protocol_tool] æ¨¡æ‹Ÿå¤±è´¥: {e}")
        return f"âŒ æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"


# ############################################################################
# # ç¬¬äºŒé˜¶æ®µï¼šæ„å»º LangGraph Agent å·¥ä½œæµ
# ############################################################################

class CodeAgentState(TypedDict):
    """
    ä»£ç ç¼–è¾‘ Agent çš„çŠ¶æ€ç®¡ç†ç±»
    
    æ­¤çŠ¶æ€ç±»ç”¨äºè¿½è¸ª Agent å·¥ä½œæµä¸­çš„æ‰€æœ‰å…³é”®ä¿¡æ¯ã€‚
    LangGraph ä½¿ç”¨è¿™ä¸ªçŠ¶æ€åœ¨ä¸åŒçš„èŠ‚ç‚¹ä¹‹é—´ä¼ é€’æ•°æ®ã€‚
    """
    # æ¶ˆæ¯å†å²ï¼šè¿½è¸ªå®Œæ•´çš„å¯¹è¯å†å²ï¼Œè¿™æ˜¯ Agent åšå†³ç­–çš„ä¸Šä¸‹æ–‡
    messages: Annotated[list, add_messages]
    
    # å½“å‰ä»£ç ï¼šä¿å­˜å½“å‰æœ€æ–°ç‰ˆæœ¬çš„ä»£ç ï¼Œä¼šåœ¨ä¿®æ”¹å·¥å…·æˆåŠŸæ‰§è¡Œåè¢«æ›´æ–°
    current_code: str


def agent_node(state: CodeAgentState):
    """
    Agent èŠ‚ç‚¹ï¼šè´Ÿè´£å†³ç­–çš„å¤§è„‘
    
    æ­¤èŠ‚ç‚¹ä¼šæ¥æ”¶æ•´ä¸ªçŠ¶æ€ï¼Œè°ƒç”¨ç»‘å®šäº†å·¥å…·çš„ LLMã€‚
    LLM çš„è¾“å‡ºå°†å†³å®šæ˜¯ç›´æ¥å›å¤ç”¨æˆ·ï¼Œè¿˜æ˜¯è°ƒç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå·¥å…·ã€‚
    
    Args:
        state: å½“å‰çš„ Agent çŠ¶æ€
        
    Returns:
        åŒ…å« LLM å“åº”æ¶ˆæ¯çš„çŠ¶æ€æ›´æ–°
    """
    print("Debug - [agent_node] Agent is thinking...")
    
    # ä¸º Agent æ·»åŠ ç³»ç»Ÿæç¤º
    system_message = HumanMessage(content="""You are an expert Opentrons protocol programming assistant. Your primary goal is to help users modify and validate their protocols. You have two powerful tools at your disposal:

1.  **`modify_code_tool`**: Use this tool for any code modification request (e.g., change, add, remove, fix, update).
2.  **`simulate_protocol_tool`**: Use this tool to verify code correctness.

**Your workflow is critical for success. Follow it strictly:**
1.  When the user asks for a code modification, your **first and only** action should be to call `modify_code_tool`.
2.  After `modify_code_tool` succeeds, the system will confirm the change. Your **next and only** action should be to call `simulate_protocol_tool` to validate the new code.
3.  After simulation, report the results to the user. If the simulation fails, analyze the error and decide whether to call `modify_code_tool` again to fix it.
4.  If the user is just asking a question, answer it directly without using tools.
5.  If a tool call fails unexpectedly, inform the user about the failure and ask for clarification or a different approach. Do not retry the same failed tool call repeatedly.

Always proceed one step at a time. Do not chain multiple tool calls in a single response.""")
    
    # å°†ç³»ç»Ÿæ¶ˆæ¯æ’å…¥åˆ°æ¶ˆæ¯å¼€å¤´ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    messages = state["messages"]
    if not messages or messages[0].content != system_message.content:
        messages = [system_message] + messages
    
    # å°†ä¸¤ä¸ªå·¥å…·ç»‘å®šåˆ° LLM
    tools = [modify_code_tool, simulate_protocol_tool]
    llm_with_tools = llm.bind_tools(tools)
    
    # è°ƒç”¨ LLMï¼Œè®©å®ƒå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
    response = llm_with_tools.invoke(messages)
    
    print(f"Debug - [agent_node] LLM å“åº”ç±»å‹: {'å·¥å…·è°ƒç”¨' if response.tool_calls else 'ç›´æ¥å›å¤'}")
    
    # è¿”å›åŒ…å« LLM å“åº”çš„çŠ¶æ€æ›´æ–°
    return {"messages": [response]}


def tool_node(state: CodeAgentState):
    """
    å·¥å…·èŠ‚ç‚¹ï¼šè´Ÿè´£æ‰§è¡Œçš„åŒæ‰‹
    
    å½“ agent_node å†³å®šè°ƒç”¨å·¥å…·æ—¶ï¼Œè¿™ä¸ªèŠ‚ç‚¹ä¼šå®é™…æ‰§è¡Œå·¥å…·ï¼Œ
    å¹¶å°†ç»“æœè¿”å›ç»™å›¾ã€‚
    
    Args:
        state: å½“å‰çš„ Agent çŠ¶æ€
        
    Returns:
        åŒ…å«å·¥å…·æ‰§è¡Œç»“æœçš„çŠ¶æ€æ›´æ–°
    """
    print("Debug - [tool_node] å¼€å§‹æ‰§è¡Œå·¥å…·...")
    
    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆåº”è¯¥æ˜¯åŒ…å«å·¥å…·è°ƒç”¨çš„ AI æ¶ˆæ¯ï¼‰
    last_message = state["messages"][-1]
    tool_calls = last_message.tool_calls if hasattr(last_message, 'tool_calls') else []
    
    if not tool_calls:
        print("Warning - [tool_node] æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç©ºç»“æœ")
        return {"messages": []}
    
    # åˆ›å»ºå·¥å…·æ˜ å°„
    tools_by_name = {
        "modify_code_tool": modify_code_tool,
        "simulate_protocol_tool": simulate_protocol_tool
    }
    
    tool_messages = []
    updated_code = state.get("current_code", "")
    
    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    for tool_call in tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        tool_call_id = tool_call["id"]
        
        print(f"Debug - [tool_node] æ‰§è¡Œå·¥å…·: {tool_name}")
        
        try:
            if tool_name == "modify_code_tool":
                # ä¿®æ”¹ä»£ç å·¥å…·ï¼šä½¿ç”¨å½“å‰ä»£ç ä½œä¸ºè¾“å…¥
                tool_args["original_code"] = updated_code
                result = tools_by_name[tool_name].invoke(tool_args)
                updated_code = result  # æ›´æ–°å½“å‰ä»£ç 
                tool_messages.append(
                    ToolMessage(
                        content="The code has been modified successfully. According to the standard workflow, the next step should be to call `simulate_protocol_tool` to verify the changes.",
                        tool_call_id=tool_call_id,
                        name=tool_name
                    )
                )
            elif tool_name == "simulate_protocol_tool":
                # æ¨¡æ‹Ÿå·¥å…·ï¼šä½¿ç”¨å½“å‰ä»£ç ä½œä¸ºè¾“å…¥
                tool_args["code_to_simulate"] = updated_code
                result = tools_by_name[tool_name].invoke(tool_args)
                tool_messages.append(
                    ToolMessage(
                        content=result,
                        tool_call_id=tool_call_id,
                        name=tool_name
                    )
                )
            else:
                # æœªçŸ¥å·¥å…·
                tool_messages.append(
                    ToolMessage(
                        content=f"é”™è¯¯ï¼šæœªçŸ¥å·¥å…· {tool_name}",
                        tool_call_id=tool_call_id,
                        name=tool_name
                    )
                )
        except Exception as e:
            print(f"Error - [tool_node] å·¥å…·æ‰§è¡Œå¤±è´¥ {tool_name}: {e}")
            tool_messages.append(
                ToolMessage(
                    content=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}",
                    tool_call_id=tool_call_id,
                    name=tool_name
                )
            )
    
    print(f"Debug - [tool_node] æ‰§è¡Œäº† {len(tool_messages)} ä¸ªå·¥å…·")
    
    # è¿”å›å·¥å…·æ¶ˆæ¯å’Œæ›´æ–°çš„ä»£ç 
    return {
        "messages": tool_messages,
        "current_code": updated_code
    }


def should_continue(state: CodeAgentState) -> Literal["tools", "__end__"]:
    """
    æ¡ä»¶è·¯ç”±å‡½æ•°ï¼šå†³å®š Agent çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨
    
    æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ï¼Œå¦‚æœæœ‰åˆ™æ‰§è¡Œå·¥å…·ï¼Œ
    å¦åˆ™ç»“æŸå¯¹è¯ã€‚
    
    Args:
        state: å½“å‰çš„ Agent çŠ¶æ€
        
    Returns:
        "tools" å¦‚æœéœ€è¦æ‰§è¡Œå·¥å…·ï¼Œ"__end__" å¦‚æœè¦ç»“æŸå¯¹è¯
    """
    last_message = state["messages"][-1]
    
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        print("Debug - [should_continue] è·¯ç”±åˆ°å·¥å…·æ‰§è¡Œ")
        return "tools"
    else:
        print("Debug - [should_continue] è·¯ç”±åˆ°ç»“æŸ")
        return "__end__"


# æ„å»ºè‡ªä¸»ä»£ç ç¼–è¾‘ Agent å›¾
def build_code_agent_graph():
    """
    æ„å»ºå¹¶ç¼–è¯‘è‡ªä¸»ä»£ç ç¼–è¾‘ Agent çš„ LangGraph å·¥ä½œæµ
    
    Returns:
        ç¼–è¯‘åçš„å›¾å¯¹è±¡ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨
    """
    print("Debug - [build_code_agent_graph] å¼€å§‹æ„å»º Agent å›¾")
    
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(CodeAgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("agent", agent_node)      # Agent æ€è€ƒèŠ‚ç‚¹
    workflow.add_node("tools", tool_node)       # å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹
    
    # å®šä¹‰å›¾çš„æµç¨‹
    workflow.add_edge(START, "agent")           # ä»å¼€å§‹èŠ‚ç‚¹åˆ° Agent
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼šAgent å†³å®šæ˜¯è°ƒç”¨å·¥å…·è¿˜æ˜¯ç»“æŸ
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",      # å¦‚æœéœ€è¦å·¥å…·ï¼Œå»å·¥å…·èŠ‚ç‚¹
            "__end__": END         # å¦‚æœå®Œæˆï¼Œç»“æŸæµç¨‹
        }
    )
    
    # å·¥å…·æ‰§è¡Œåå›åˆ° Agent ç»§ç»­æ€è€ƒ
    workflow.add_edge("tools", "agent")
    
    # ç¼–è¯‘å›¾
    graph = workflow.compile()
    print("Debug - [build_code_agent_graph] Agent å›¾æ„å»ºå®Œæˆ")
    
    return graph


# åˆå§‹åŒ–å…¨å±€çš„ä»£ç ç¼–è¾‘ Agent
code_agent_graph = build_code_agent_graph()


# ############################################################################
# # å¯¹è¯å¼ç¼–è¾‘åŠŸèƒ½ (ä¿ç•™ç°æœ‰åŠŸèƒ½)
# ############################################################################

def _edit_sop_with_diff(original_sop: str, user_instruction: str, hardware_context: str) -> str:
    """
    (å†…éƒ¨å‡½æ•°) ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆdiffå¹¶åº”ç”¨å®ƒæ¥ä¿®æ”¹SOPã€‚
    
    å‚æ•°:
        original_sop (str): åŸå§‹SOPæ–‡æœ¬ã€‚
        user_instruction (str): ç”¨æˆ·çš„ä¿®æ”¹æŒ‡ä»¤ã€‚
        hardware_context (str): ç¡¬ä»¶é…ç½®ï¼Œç”¨äºæä¾›ä¸Šä¸‹æ–‡ã€‚
        
    è¿”å›:
        str: ä¿®æ”¹åçš„æ–°SOPã€‚
    """
    try:
        print(f"Debug - [edit_sop_with_diff] å¼€å§‹ä¸ºSOPç”Ÿæˆdiff")
        
        # å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„æ¨¡æ¿å’Œå·¥å…·
        from backend.prompts import SOP_EDIT_DIFF_PROMPT_TEMPLATE
        from langchain_core.prompts import PromptTemplate
        from langchain.chains import LLMChain
        
        # 1. å‡†å¤‡è°ƒç”¨å¤§æ¨¡å‹çš„è¾“å…¥
        prompt = PromptTemplate(
            input_variables=["original_sop", "user_instruction", "hardware_context"],
            template=SOP_EDIT_DIFF_PROMPT_TEMPLATE
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        
        # 2. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆdiff
        diff_output = chain.run({
            "original_sop": original_sop,
            "user_instruction": user_instruction,
            "hardware_context": hardware_context
        })
        
        print(f"Debug - [edit_sop_with_diff] LLMç”Ÿæˆçš„SOP Diffå†…å®¹:\n---\n{diff_output}\n---")
        
        if not diff_output or not "------- SEARCH" in diff_output:
            print("Warning - LLM did not return a valid diff. Returning original SOP.")
            raise ValueError("AI did not produce a valid modification for the SOP. Please try rephrasing your request.")

        # 3. åº”ç”¨diff
        print(f"Debug - [edit_sop_with_diff] åº”ç”¨diffå‰çš„SOPé•¿åº¦: {len(original_sop)}")
        new_sop = apply_diff(original_sop, diff_output)
        print(f"Debug - [edit_sop_with_diff] åº”ç”¨diffåçš„SOPé•¿åº¦: {len(new_sop)}")
        
        print(f"Debug - [edit_sop_with_diff] SOP Diffåº”ç”¨æˆåŠŸï¼ŒSOPå·²ä¿®æ”¹ã€‚")
        
        return new_sop

    except ValueError as ve:
        print(f"Error - [edit_sop_with_diff] åº”ç”¨SOP diffæ—¶å‡ºé”™: {ve}")
        raise ve # é‡æ–°æŠ›å‡ºï¼Œè®©è°ƒç”¨è€…çŸ¥é“æ˜¯diffåº”ç”¨é—®é¢˜
    except Exception as e:
        print(f"Error - [edit_sop_with_diff] ç¼–è¾‘SOPæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        import traceback
        error_traceback = traceback.format_exc()
        print(f"Debug - [edit_sop_with_diff] å®Œæ•´é”™è¯¯å †æ ˆ:\n{error_traceback}")
        raise RuntimeError(f"An unexpected error occurred while editing the SOP: {e}")


def converse_about_sop(original_sop: str, user_instruction: str, hardware_context: str) -> Dict[str, str]:
    """
    å¤„ç†å…³äºSOPçš„å¯¹è¯ï¼Œå¯èƒ½æ˜¯ç¼–è¾‘æŒ‡ä»¤æˆ–æ™®é€šèŠå¤©ã€‚
    
    è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ç±»å‹å’Œå†…å®¹ã€‚
    """
    print(f"Debug - [converse_about_sop] Classifying user instruction: '{user_instruction}'")
    intent = _classify_sop_intent(user_instruction)
    print(f"Debug - [converse_about_sop] Classified intent as: '{intent}'")
    
    if intent == "edit":
        try:
            modified_sop = _edit_sop_with_diff(original_sop, user_instruction, hardware_context)
            # ç»Ÿä¸€å“åº”æ ¼å¼ï¼Œä½¿ç”¨ 'content' ä½œä¸ºé”®
            return {"type": "edit", "content": modified_sop}
        except ValueError as e:
            # ä¸“é—¨å¤„ç†diffåº”ç”¨å¤±è´¥çš„æƒ…å†µ
            print(f"Warning - [converse_about_sop] Diff application failed: {e}")
            error_message = f"I tried to edit the SOP, but couldn't apply the changes. This can happen if the instruction is ambiguous. Please try rephrasing. (Error: {str(e)})"
            return {"type": "chat", "content": error_message}
        except Exception as e:
            # å¤„ç†å…¶ä»–æ‰€æœ‰æœªçŸ¥é”™è¯¯
            print(f"Error - [converse_about_sop] An unexpected error occurred: {e}")
            error_message = f"I encountered an unexpected server error while trying to edit the SOP. Please try again. (Details: {str(e)})"
            return {"type": "chat", "content": error_message}
    else: # intent == "chat"
        chat_response = _chat_about_sop(original_sop, user_instruction)
        # ç»Ÿä¸€å“åº”æ ¼å¼ï¼Œä½¿ç”¨ 'content' ä½œä¸ºé”®
        return {"type": "chat", "content": chat_response}


def _classify_sop_intent(user_instruction: str) -> str:
    """
    (å†…éƒ¨å‡½æ•°) å¯¹ç”¨æˆ·çš„SOPç›¸å…³æŒ‡ä»¤è¿›è¡Œæ„å›¾åˆ†ç±»ã€‚
    """
    try:
        import json
        import re
        from backend.prompts import SOP_CONVERSATION_CLASSIFIER_PROMPT_TEMPLATE
        from langchain_core.prompts import PromptTemplate
        from langchain.chains import LLMChain

        # Use specialized, faster model for intent classification
        intent_llm = ChatOpenAI(
            model_name=DEEPSEEK_INTENT_MODEL,
            openai_api_base=DEEPSEEK_BASE_URL,
            openai_api_key=DEEPSEEK_API_KEY,
            temperature=0.0,
            max_retries=1,
            request_timeout=20
        )

        prompt = PromptTemplate(
            input_variables=["user_instruction"],
            template=SOP_CONVERSATION_CLASSIFIER_PROMPT_TEMPLATE
        )
        chain = LLMChain(llm=intent_llm, prompt=prompt)
        
        response_str = chain.run({"user_instruction": user_instruction})
        
        # å¢å¼ºçš„JSONè§£æ
        try:
            # 1. å°è¯•ç›´æ¥è§£æ
            response_json = json.loads(response_str)
        except json.JSONDecodeError:
            # 2. å¦‚æœå¤±è´¥ï¼Œå°è¯•ä»Markdownä»£ç å—ä¸­æå–
            match = re.search(r'```json\s*(\{.*?\})\s*```', response_str, re.DOTALL)
            if not match:
                # 3. å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•åœ¨æ•´ä¸ªå­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾JSONå¯¹è±¡
                match = re.search(r'(\{.*?\})', response_str, re.DOTALL)
            
            if match:
                response_json = json.loads(match.group(1))
            else:
                # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼Œç”±å¤–éƒ¨catchå—å¤„ç†
                raise ValueError("No valid JSON found in the response.")

        intent = response_json.get("intent", "chat")
        
        if intent not in ["edit", "chat"]:
            return "chat" 
            
        return intent
    except Exception as e:
        print(f"Error during intent classification, defaulting to 'chat': {e}")
        # é™çº§ï¼šå¦‚æœJSONè§£æå’Œæå–éƒ½å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯è¿›è¡Œåˆ¤æ–­
        edit_keywords = [
            'change', 'add', 'remove', 'replace', 'modify', 'update', 'use', 'delete', 'make', 
            'æ”¹', 'å¢åŠ ', 'æ·»åŠ ', 'åˆ é™¤', 'æ›¿æ¢', 'ä¿®æ”¹', 'æ›´æ–°', 'ä½¿ç”¨', 'è®¾ä¸º', 'æ¢æˆ', 'å˜ä¸º'
        ]
        if any(keyword in user_instruction.lower() for keyword in edit_keywords):
            print("Info: JSON parsing failed, but keyword matching classified as 'edit'.")
            return "edit"
        return "chat"


def _classify_code_intent(user_instruction: str) -> str:
    """
    (Internal) Classifies the user's intent for a code-related instruction.
    """
    try:
        import json
        import re
        from backend.prompts import CODE_CONVERSATION_CLASSIFIER_PROMPT_TEMPLATE
        from langchain_core.prompts import PromptTemplate
        from langchain.chains import LLMChain

        # Use specialized, faster model for intent classification
        intent_llm = ChatOpenAI(
            model_name=DEEPSEEK_INTENT_MODEL,
            openai_api_base=DEEPSEEK_BASE_URL,
            openai_api_key=DEEPSEEK_API_KEY,
            temperature=0.0,
            max_retries=1,
            request_timeout=20
        )

        prompt = PromptTemplate(
            input_variables=["user_instruction"],
            template=CODE_CONVERSATION_CLASSIFIER_PROMPT_TEMPLATE
        )
        chain = LLMChain(llm=intent_llm, prompt=prompt)
        
        response_str = chain.run({"user_instruction": user_instruction})
        
        # Enhanced JSON parsing
        try:
            response_json = json.loads(response_str)
        except json.JSONDecodeError:
            match = re.search(r'```json\s*(\{.*?\})\s*```', response_str, re.DOTALL)
            if not match:
                match = re.search(r'(\{.*?\})', response_str, re.DOTALL)
            
            if match:
                response_json = json.loads(match.group(1))
            else:
                raise ValueError("No valid JSON found in the response.")

        intent = response_json.get("intent", "chat")
        
        return intent if intent in ["edit", "chat"] else "chat"
    except Exception as e:
        print(f"Error during code intent classification, defaulting to 'chat': {e}")
        # Fallback to keyword matching
        edit_keywords = [
            'change', 'add', 'remove', 'replace', 'modify', 'update', 'use', 'delete', 'make', 
            'æ”¹', 'å¢åŠ ', 'æ·»åŠ ', 'åˆ é™¤', 'æ›¿æ¢', 'ä¿®æ”¹', 'æ›´æ–°', 'ä½¿ç”¨', 'è®¾ä¸º', 'æ¢æˆ', 'å˜ä¸º'
        ]
        if any(keyword in user_instruction.lower() for keyword in edit_keywords):
            print("Info: JSON parsing failed, but keyword matching classified as 'edit' for code.")
            return "edit"
        return "chat"

def _chat_about_code(original_code: str, user_instruction: str) -> str:
    """
    (Internal) Handles a general chat conversation about the code.
    """
    try:
        from backend.prompts import GENERAL_CODE_CHAT_PROMPT_TEMPLATE
        from langchain_core.prompts import PromptTemplate
        from langchain.chains import LLMChain

        prompt = PromptTemplate(
            input_variables=["original_code", "user_instruction"],
            template=ENG_GENERAL_CODE_CHAT_PROMPT_TEMPLATE
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        
        response = chain.run({
            "original_code": original_code,
            "user_instruction": user_instruction
        })
        return response
    except Exception as e:
        print(f"Error during general code chat: {e}")
        return "Sorry, I encountered an error while trying to respond."

def converse_about_code(original_code: str, user_instruction: str) -> Dict[str, str]:
    """
    ä½¿ç”¨è‡ªä¸» LangGraph Agent å¤„ç†ä»£ç ç›¸å…³çš„å¯¹è¯
    
    è¿™ä¸ªé‡æ„åçš„å‡½æ•°ç”¨æ–°çš„ Agent æ›¿æ¢äº†æ—§çš„å›ºåŒ–é€»è¾‘ï¼Œ
    Agent å¯ä»¥è‡ªä¸»å†³å®šæ˜¯å¦éœ€è¦ä¿®æ”¹ä»£ç ã€è¿è¡Œæ¨¡æ‹Ÿï¼Œæˆ–åªæ˜¯å›ç­”é—®é¢˜ã€‚
    
    Args:
        original_code: åŸå§‹ä»£ç 
        user_instruction: ç”¨æˆ·æŒ‡ä»¤
        
    Returns:
        åŒ…å«ç±»å‹å’Œå†…å®¹çš„å­—å…¸
    """
    print(f"Debug - [converse_about_code] Processing user instruction with autonomous agent")
    
    try:
        # Initialize Agent state
        initial_state = CodeAgentState(
            messages=[
                HumanMessage(content=f"""I have the following Opentrons protocol. Please help me with the user's request.

User Request: {user_instruction}

Current Code:
```python
{original_code}
```

Please analyze the user's request. If it requires modifying the code, use the `modify_code_tool`. If it requires validating the code, use the `simulate_protocol_tool`. If it's a general question, answer it directly. You can call multiple tools as needed to complete the task.""")
            ],
            current_code=original_code
        )
        
        # Invoke the Agent graph
        print("Debug - [converse_about_code] Starting Agent execution")
        final_state = code_agent_graph.invoke(initial_state)
        
        # åˆ†æ Agent çš„æœ€ç»ˆå“åº”
        last_message = final_state["messages"][-1]
        final_code = final_state.get("current_code", original_code)
        
        print(f"Debug - [converse_about_code] Agent æ‰§è¡Œå®Œæˆ")
        
        # åˆ¤æ–­æ˜¯å¦æœ‰ä»£ç ä¿®æ”¹
        if final_code != original_code:
            # ä»£ç è¢«ä¿®æ”¹äº†ï¼Œè¿”å›ç¼–è¾‘ç±»å‹
            return {
                "type": "edit", 
                "content": final_code
            }
        else:
            # æ²¡æœ‰ä»£ç ä¿®æ”¹ï¼Œè¿”å›èŠå¤©ç±»å‹
            if hasattr(last_message, 'content'):
                response_content = last_message.content
            else:
                response_content = "Task completed."
            
            return {
                "type": "chat",
                "content": response_content
            }
            
    except Exception as e:
        print(f"Error - [converse_about_code] Agent execution failed: {e}")
        import traceback
        traceback.print_exc()
        
        # Fallback to a simple reply
        error_message = f"I'm sorry, I encountered a problem while processing your request. Error details: {str(e)}\n\nPlease try rephrasing your request or ensure the code is formatted correctly."
        return {"type": "chat", "content": error_message}


def _validate_python_syntax(code: str) -> tuple[bool, str]:
    """
    å¿«é€ŸéªŒè¯Pythonä»£ç è¯­æ³•æ˜¯å¦æ­£ç¡®ã€‚
    
    Args:
        code: è¦æ£€æŸ¥çš„Pythonä»£ç å­—ç¬¦ä¸²
        
    Returns:
        (is_valid, error_message): å¦‚æœè¯­æ³•æ­£ç¡®è¿”å›(True, "")ï¼Œå¦åˆ™è¿”å›(False, é”™è¯¯ä¿¡æ¯)
    """
    try:
        ast.parse(code)
        return True, ""
    except SyntaxError as e:
        error_msg = f"Pythonè¯­æ³•é”™è¯¯: {e.msg} (ç¬¬{e.lineno}è¡Œ"
        if e.offset:
            error_msg += f", ç¬¬{e.offset}åˆ—"
        error_msg += ")"
        if e.text:
            error_msg += f"\né—®é¢˜è¡Œ: {e.text.strip()}"
        return False, error_msg
    except Exception as e:
        return False, f"ä»£ç è§£æå¼‚å¸¸: {str(e)}"


# æ—§çš„ _edit_and_validate_code_with_retries å‡½æ•°å·²è¢«æ–°çš„ Agent æ¶æ„å–ä»£
# NOTE: Removed deprecated functions _edit_and_validate_code_with_retries and edit_code_with_diff
# These have been replaced by the new conversational code editing system using LangGraph agents

async def converse_about_code_stream(original_code: str, user_instruction: str):
    """
    Handles conversational code edits via a real-time stream.
    Yields events for agent thoughts, tool calls, and final results.
    """
    print(f"Debug - [converse_about_code_stream] Starting stream for: {user_instruction}")
    
    # Initialize Agent state
    initial_state = CodeAgentState(
        messages=[
            HumanMessage(content=f"""I have the following Opentrons protocol. Please help me with the user's request.

User Request: {user_instruction}

Current Code:
```python
{original_code}
```

Please analyze the user's request. If it requires modifying the code, use the `modify_code_tool`. If it requires validating the code, use the `simulate_protocol_tool`. If it's a general question, answer it directly. You can call multiple tools as needed to complete the task.""")
        ],
        current_code=original_code
    )
    
    current_state = initial_state
    
    try:
        async for chunk in code_agent_graph.astream(initial_state):
            for node_name, node_output in chunk.items():
                current_state.update(node_output)
                
                if node_name == "agent":
                    yield {
                        "event_type": "thought",
                        "message": "The agent is thinking about the next step..."
                    }
                    # Check if the agent is calling a tool
                    last_message = current_state["messages"][-1]
                    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                         yield {
                            "event_type": "tool_call",
                            "tool_name": last_message.tool_calls[0]['name'],
                            "message": f"Decided to call tool: `{last_message.tool_calls[0]['name']}`"
                        }

                elif node_name == "tools":
                    last_tool_message = current_state["messages"][-1]
                    yield {
                        "event_type": "tool_result",
                        "tool_name": last_tool_message.name,
                        "content": last_tool_message.content,
                        "message": f"Tool `{last_tool_message.name}` finished execution."
                    }
        
        # Final result analysis
        final_code = current_state.get("current_code", original_code)
        if final_code != original_code:
            final_content = final_code
            result_type = "edit"
        else:
            final_content = current_state["messages"][-1].content
            result_type = "chat"
            
        yield {
            "event_type": "final_result",
            "type": result_type,
            "content": final_content,
            "message": "Agent has finished the task."
        }

    except Exception as e:
        print(f"Error - [converse_about_code_stream] Stream failed: {e}")
        yield {
            "event_type": "error",
            "message": str(e)
        }